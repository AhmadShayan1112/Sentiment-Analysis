{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "koI9bHBwIcaW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\ahmed\\urdu_sarcastic_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrUFRGcZCbl-",
    "outputId": "fdc66fe0-5675-4660-d82e-63cc13cf21ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lughaatNLP in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (1.13.1)\n",
      "Requirement already satisfied: gtts in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (2.5.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (3.10.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (0.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->lughaatNLP) (2.32.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->lughaatNLP) (8.1.7)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from python-Levenshtein->lughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from Levenshtein==0.26.0->python-Levenshtein->lughaatNLP) (3.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->lughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->lughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from SpeechRecognition->lughaatNLP) (4.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow->lughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts->lughaatNLP) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install lughaatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yYqyuaBpV-yD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import LughaatNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "rsgKmdzcWAXn",
    "outputId": "872b2a37-9c94-416c-91e0-851692c360ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20060/20060 [32:55<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
      "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
      "\n",
      "                                      corrected_text  \n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒ...  \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³...  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
      "3                                      Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†Ù„ ğŸ˜  \n",
      "4  `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ...  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df = pd.read_csv('urdu_sarcastic_dataset.csv')\n",
    "df['urdu_text'] = df['urdu_text'].fillna('').astype(str)\n",
    "spell_checker = LughaatNLP.LughaatNLP()\n",
    "def process_sentence(sentence):\n",
    "    return spell_checker.corrected_sentence_spelling(sentence, 60)\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "df['corrected_text'] = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_sentence)(sentence) for sentence in tqdm(df['urdu_text'])\n",
    ")\n",
    "print(df[['urdu_text', 'corrected_text']].head())\n",
    "df.to_csv('urdu_sarcastic_dataset_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcYYDg8PCFc8",
    "outputId": "0de573e6-bc5d-4f93-b590-ca3e8c794314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: LughaatNLP in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (1.13.1)\n",
      "Requirement already satisfied: gtts in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (2.5.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (3.10.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (0.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->LughaatNLP) (2.32.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->LughaatNLP) (8.1.7)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from python-Levenshtein->LughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from Levenshtein==0.26.0->python-Levenshtein->LughaatNLP) (3.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->LughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->LughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from SpeechRecognition->LughaatNLP) (4.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow->LughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts->LughaatNLP) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install LughaatNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NAN Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PlVxJMqWI6qh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('urdu_sarcastic_dataset_corrected.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df = df.dropna(subset=['urdu_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "oLlk9NL0TRHp",
    "outputId": "e1e3e78d-33ea-4acb-dbb3-e2ae5cb37d0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†Ù„ ğŸ˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø§Ù†Ø³Ø§Ù† Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø­Ø§Ù…ÛŒ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ÛŒØ§Ø± Ú†Ø§Ø±Û ÙˆÛŒÙ„Ù…Ø§ ÛÙˆÙ†Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÚ¯ÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ØªØ³Ù„ÛŒ Ù„Ú‘Ø§Ø¦ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§Ø²ÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ù„Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ù„Ø§Ù„Ùˆ Ø¯ÙˆØ³ØªÙˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ Ø±ÙˆÙ¾Û’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ Ø²Ø®Ù… Ú©Ú¾Ø§ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§ Ú¯Û’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  is_sarcastic  \\\n",
       "0   ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
       "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
       "3                                        Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
       "5         Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
       "6       Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€           0.0   \n",
       "7                               Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š           0.0   \n",
       "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...           1.0   \n",
       "9            ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚           1.0   \n",
       "10                      ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚           0.0   \n",
       "11                       Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„           0.0   \n",
       "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...           1.0   \n",
       "13   ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’           1.0   \n",
       "14                                Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚           0.0   \n",
       "\n",
       "                                       corrected_text  \n",
       "0   ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒ...  \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³...  \n",
       "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†Ù„ ğŸ˜  \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ...  \n",
       "5         Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥  \n",
       "6       Ø§Ù†Ø³Ø§Ù† Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€  \n",
       "7                                 Ø­Ø§Ù…ÛŒ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†  \n",
       "8   ÛŒØ§Ø± Ú†Ø§Ø±Û ÙˆÛŒÙ„Ù…Ø§ ÛÙˆÙ†Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÚ¯ÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...  \n",
       "9            ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚  \n",
       "10                      ØªØ³Ù„ÛŒ Ù„Ú‘Ø§Ø¦ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§Ø²ÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚  \n",
       "11                       Ù„Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„  \n",
       "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ù„Ø§Ù„Ùˆ Ø¯ÙˆØ³ØªÙˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ Ø±ÙˆÙ¾Û’ ...  \n",
       "13        ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ Ø²Ø®Ù… Ú©Ú¾Ø§ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§ Ú¯Û’  \n",
       "14                                Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Puntion , Stop words , Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "oKG6m7i0I-cF",
    "outputId": "6ea82bac-83b8-4d55-9582-478ae228e9c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³ØªÛŒ Ø¢Ø¤Úº Ù…ÛŒÚº</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†Ù„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø§ÛŒØ³  Ø­Ø§Ù…ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø§Ù†Ø³Ø§Ù† ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø­Ø§Ù…ÛŒ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ÛŒØ§Ø± Ú†Ø§Ø±Û ÙˆÛŒÙ„Ù…Ø§ Ø¢Ø±Û’ ÛÙˆÚ¯ÛŒØ§ ÛÛ’ ØªØ³Ù„ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†ÙˆÙ…ÛŒ Ù…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ØªØ³Ù„ÛŒ Ù„Ú‘Ø§Ø¦ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§Ø²ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ù„Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ù„Ø§Ù„Ùˆ Ø¯ÙˆØ³ØªÙˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ Ø±ÙˆÙ¾Û’ Ú¯Ø² ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ Ø²Ø®Ù… Ú©Ú¾Ø§ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§ Ú¯Û’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ú†ÙˆÙ†Ø§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  is_sarcastic  \\\n",
       "0   ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
       "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
       "3                                        Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
       "5         Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
       "6       Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€           0.0   \n",
       "7                               Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š           0.0   \n",
       "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...           1.0   \n",
       "9            ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚           1.0   \n",
       "10                      ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚           0.0   \n",
       "11                       Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„           0.0   \n",
       "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...           1.0   \n",
       "13   ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’           1.0   \n",
       "14                                Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚           0.0   \n",
       "\n",
       "                                       corrected_text  \n",
       "0                 Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’  \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³ØªÛŒ Ø¢Ø¤Úº Ù…ÛŒÚº  \n",
       "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...  \n",
       "3                                         Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ†Ù„  \n",
       "4                      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø§ÛŒØ³  Ø­Ø§Ù…ÛŒ  \n",
       "5                             Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±  \n",
       "6                                Ø§Ù†Ø³Ø§Ù† ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±  \n",
       "7                                      Ø­Ø§Ù…ÛŒ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†  \n",
       "8   ÛŒØ§Ø± Ú†Ø§Ø±Û ÙˆÛŒÙ„Ù…Ø§ Ø¢Ø±Û’ ÛÙˆÚ¯ÛŒØ§ ÛÛ’ ØªØ³Ù„ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†ÙˆÙ…ÛŒ Ù…...  \n",
       "9                       Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’  \n",
       "10                             ØªØ³Ù„ÛŒ Ù„Ú‘Ø§Ø¦ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§Ø²ÛŒ  \n",
       "11                             Ù„Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’  \n",
       "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ù„Ø§Ù„Ùˆ Ø¯ÙˆØ³ØªÙˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ Ø±ÙˆÙ¾Û’ Ú¯Ø² ...  \n",
       "13                     Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ Ø²Ø®Ù… Ú©Ú¾Ø§ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§ Ú¯Û’  \n",
       "14                                               Ú†ÙˆÙ†Ø§  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "stop_words = frozenset(\"\"\"\n",
    "Ø¢ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒÚº Ø¢Ø¦Û’ Ø¢ØªØ§ Ø¢ØªÛŒ Ø¢ØªÛ’ Ø¢Ø¯Ø§Ø¨ Ø¢Ø¯Ú¾ Ø¢Ø¯Ú¾Ø§ Ø¢Ø¯Ú¾ÛŒ Ø¢Ø¯Ú¾Û’ Ø¢Ø³\n",
    " Ø¢Ù…Ø¯ÛŒØ¯ Ø¢Ù†Ø§ Ø¢Ù†Ø³Û Ø¢Ù†ÛŒ Ø¢Ù†Û’ Ø¢Ù¾ Ø¢Ú¯Û’ Ø¢Û Ø¢ÛØ§ Ø¢ÛŒØ§ Ø§Ø¨ Ø§Ø¨Ú¾ÛŒ Ø§Ø¨Û’\n",
    " Ø§ØªÙˆØ§Ø± Ø§Ø±Ø¨ Ø§Ø±Ø¨ÙˆÛŒÚº Ø§Ø±Û’ Ø§Ø³ Ø§Ø³Ú©Ø§ Ø§Ø³Ú©ÛŒ Ø§Ø³Ú©Û’ Ø§Ø³ÛŒ Ø§Ø³Û’ Ø§Ù Ø§ÙÙˆÛ Ø§Ù„Ø§ÙˆÙ„ Ø§Ù„Ø¨ØªÛ\n",
    " Ø§Ù„Ø«Ø§Ù†ÛŒ Ø§Ù„Ø­Ø±Ø§Ù… Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù Ø§Ù„Ù…Ú©Ø±Ù… Ø§Ù† Ø§Ù†Ø¯Ø± Ø§Ù†Ú©Ø§ Ø§Ù†Ú©ÛŒ Ø§Ù†Ú©Û’ Ø§Ù†ÛÙˆÚº Ø§Ù†ÛÛŒ Ø§Ù†ÛÛŒÚº\n",
    " Ø§ÙˆØ¦Û’ Ø§ÙˆØ± Ø§ÙˆÙ¾Ø± Ø§ÙˆÛÙˆ Ø§Ù¾ Ø§Ù¾Ù†Ø§ Ø§Ù¾Ù†ÙˆÚº Ø§Ù¾Ù†ÛŒ Ø§Ù¾Ù†Û’ Ø§Ù¾Ù†Û’Ø¢Ù¾ Ø§Ú©Ø¨Ø± Ø§Ú©Ø«Ø± Ø§Ú¯Ø± Ø§Ú¯Ø±Ú†Û\n",
    " Ø§Ú¯Ø³Øª Ø§ÛØ§ÛØ§ Ø§ÛŒØ³Ø§ Ø§ÛŒØ³ÛŒ Ø§ÛŒØ³Û’ Ø§ÛŒÚ© Ø¨Ø§Ø¦ÛŒÚº Ø¨Ø§Ø± Ø¨Ø§Ø±Û’ Ø¨Ø§Ù„Ú©Ù„ Ø¨Ø§ÙˆØ¬ÙˆØ¯ Ø¨Ø§ÛØ± Ø¨Ø¬ Ø¨Ø¬Û’\n",
    " Ø¨Ø®ÛŒØ± Ø¨Ø±Ø³Ø§Øª Ø¨Ø´Ø±Ø·ÛŒÚ©Û Ø¨Ø¹Ø¶ Ø¨ØºÛŒØ± Ø¨Ù„Ú©Û Ø¨Ù† Ø¨Ù†Ø§ Ø¨Ù†Ø§Ø¤ Ø¨Ù†Ø¯ Ø¨Ú‘ÛŒ Ø¨Ú¾Ø± Ø¨Ú¾Ø±ÛŒÚº\n",
    " Ø¨Ú¾ÛŒ Ø¨ÛØ§Ø± Ø¨ÛØª Ø¨ÛØªØ± Ø¨ÛŒÚ¯Ù… ØªØ§Ú©Û ØªØ§ÛÙ… ØªØ¨ ØªØ¬Ú¾ ØªØ¬Ú¾ÛŒ ØªØ¬Ú¾Û’ ØªØ±Ø§ ØªØ±ÛŒ\n",
    " ØªÙ„Ú© ØªÙ… ØªÙ…Ø§Ù… ØªÙ…ÛØ§Ø±Ø§ ØªÙ…ÛØ§Ø±ÙˆÚº ØªÙ…ÛØ§Ø±ÛŒ ØªÙ…ÛØ§Ø±Û’ ØªÙ…ÛÛŒÚº ØªÙˆ ØªÚ© ØªÚ¾Ø§ ØªÚ¾ÛŒ ØªÚ¾ÛŒÚº ØªÚ¾Û’\n",
    " ØªÛØ§Ø¦ÛŒ ØªÛŒØ±Ø§ ØªÛŒØ±ÛŒ ØªÛŒØ±Û’ ØªÛŒÙ† Ø¬Ø§ Ø¬Ø§Ø¤ Ø¬Ø§Ø¦ÛŒÚº Ø¬Ø§Ø¦Û’ Ø¬Ø§ØªØ§ Ø¬Ø§ØªÛŒ Ø¬Ø§ØªÛ’ Ø¬Ø§Ù†ÛŒ Ø¬Ø§Ù†Û’\n",
    " Ø¬Ø¨ Ø¬Ø¨Ú©Û Ø¬Ø¯Ú¾Ø± Ø¬Ø³ Ø¬Ø³Û’ Ø¬Ù† Ø¬Ù†Ø§Ø¨ Ø¬Ù†ÛÙˆÚº Ø¬Ù†ÛÛŒÚº Ø¬Ùˆ Ø¬ÛØ§Úº Ø¬ÛŒ Ø¬ÛŒØ³Ø§\n",
    " Ø¬ÛŒØ³ÙˆÚº Ø¬ÛŒØ³ÛŒ Ø¬ÛŒØ³Û’ Ø¬ÛŒÙ¹Ú¾ Ø­Ø§Ù„Ø§Ù†Ú©Û Ø­Ø§Ù„Ø§Úº Ø­ØµÛ Ø­Ø¶Ø±Øª Ø®Ø§Ø·Ø± Ø®Ø§Ù„ÛŒ Ø®Ø¯Ø§ Ø®Ø²Ø§Úº Ø®ÙˆØ§Û Ø®ÙˆØ¨\n",
    " Ø®ÙˆØ¯ Ø¯Ø§Ø¦ÛŒÚº Ø¯Ø±Ù…ÛŒØ§Ù† Ø¯Ø±ÛŒÚº Ø¯Ùˆ Ø¯ÙˆØ±Ø§Ù† Ø¯ÙˆØ³Ø±Ø§ Ø¯ÙˆØ³Ø±ÙˆÚº Ø¯ÙˆØ³Ø±ÛŒ Ø¯ÙˆØ´Ù†Ø¨Û Ø¯ÙˆÚº Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ø¯Ú¯Ù†Ø§ Ø¯ÛŒ\n",
    " Ø¯ÛŒØ¦Û’ Ø¯ÛŒØ§ Ø¯ÛŒØªØ§ Ø¯ÛŒØªÛŒ Ø¯ÛŒØªÛ’ Ø¯ÛŒØ± Ø¯ÛŒÙ†Ø§ Ø¯ÛŒÙ†ÛŒ Ø¯ÛŒÙ†Û’ Ø¯ÛŒÚ©Ú¾Ùˆ Ø¯ÛŒÚº Ø¯ÛŒÛ’ Ø¯Û’ Ø°Ø±ÛŒØ¹Û’\n",
    " Ø±Ú©Ú¾Ø§ Ø±Ú©Ú¾ØªØ§ Ø±Ú©Ú¾ØªÛŒ Ø±Ú©Ú¾ØªÛ’ Ø±Ú©Ú¾Ù†Ø§ Ø±Ú©Ú¾Ù†ÛŒ Ø±Ú©Ú¾Ù†Û’ Ø±Ú©Ú¾Ùˆ Ø±Ú©Ú¾ÛŒ Ø±Ú©Ú¾Û’ Ø±Û Ø±ÛØ§ Ø±ÛØªØ§\n",
    " Ø±ÛØªÛŒ Ø±ÛØªÛ’ Ø±ÛÙ†Ø§ Ø±ÛÙ†ÛŒ Ø±ÛÙ†Û’ Ø±ÛÙˆ Ø±ÛÛŒ Ø±ÛÛŒÚº Ø±ÛÛ’ Ø³Ø§ØªÚ¾ Ø³Ø§Ù…Ù†Û’ Ø³Ø§Ú‘Ú¾Û’ Ø³Ø¨ Ø³Ø¨Ú¾ÛŒ\n",
    " Ø³Ø±Ø§Ø³Ø± Ø³Ù„Ø§Ù… Ø³Ù…ÛŒØª Ø³ÙˆØ§ Ø³ÙˆØ§Ø¦Û’ Ø³Ú©Ø§ Ø³Ú©ØªØ§ Ø³Ú©ØªÛ’ Ø³Û Ø³ÛÛŒ Ø³ÛŒ Ø³Û’ Ø´Ø§Ù… Ø´Ø§ÛŒØ¯\n",
    " Ø´Ú©Ø±ÛŒÛ ØµØ§Ø­Ø¨ ØµØ§Ø­Ø¨Û ØµØ±Ù Ø¶Ø±ÙˆØ± Ø·Ø±Ø­ Ø·Ø±Ù Ø·ÙˆØ± Ø¹Ù„Ø§ÙˆÛ Ø¹ÛŒÙ† ÙØ±ÙˆØ±ÛŒ ÙÙ‚Ø· ÙÙ„Ø§Úº\n",
    " ÙÛŒ Ù‚Ø¨Ù„ Ù‚Ø·Ø§ Ù„Ø§Ø¦ÛŒ Ù„Ø§Ø¦Û’ Ù„Ø§ØªØ§ Ù„Ø§ØªÛŒ Ù„Ø§ØªÛ’ Ù„Ø§Ù†Ø§ Ù„Ø§Ù†ÛŒ Ù„Ø§ÛŒØ§ Ù„Ùˆ Ù„ÙˆØ¬ÛŒ Ù„ÙˆÚ¯ÙˆÚº\n",
    " Ù„Ú¯ Ù„Ú¯Ø§ Ù„Ú¯ØªØ§ Ù„Ú¯ØªÛŒ Ù„Ú¯ÛŒ Ù„Ú¯ÛŒÚº Ù„Ú¯Û’ Ù„ÛØ°Ø§ Ù„ÛŒ Ù„ÛŒØ§ Ù„ÛŒØªØ§ Ù„ÛŒØªÛŒ Ù„ÛŒØªÛ’ Ù„ÛŒÚ©Ù†\n",
    " Ù„ÛŒÚº Ù„ÛŒÛ’ Ù„Û’ Ù…Ø§Ø³ÙˆØ§ Ù…Øª Ù…Ø¬Ú¾ Ù…Ø¬Ú¾ÛŒ Ù…Ø¬Ú¾Û’ Ù…Ø­ØªØ±Ù… Ù…Ø­ØªØ±Ù…ÛŒ Ù…Ø­Ø¶ Ù…Ø±Ø§ Ù…Ø±Ø­Ø¨Ø§\n",
    " Ù…Ø±ÛŒ Ù…Ø±Û’ Ù…Ø²ÛŒØ¯ Ù…Ø³ Ù…Ø³Ø² Ù…Ø³Ù¹Ø± Ù…Ø·Ø§Ø¨Ù‚ Ù…Ø·Ù„Ù‚ Ù…Ù„ Ù…Ù†Ù¹ Ù…Ù†Ù¹ÙˆÚº Ù…Ú©Ø±Ù…ÛŒ Ù…Ú¯Ø±\n",
    " Ù…Ú¯Ú¾Ø± Ù…ÛØ±Ø¨Ø§Ù†ÛŒ Ù…ÛŒØ±Ø§ Ù…ÛŒØ±ÙˆÚº Ù…ÛŒØ±ÛŒ Ù…ÛŒØ±Û’ Ù…ÛŒÚº Ù†Ø§ Ù†Ø²Ø¯ÛŒÚ© Ù†Ù…Ø§ Ù†Ùˆ Ù†ÙˆÙ…Ø¨Ø± Ù†Û\n",
    " Ù†ÛŒØ² Ù†ÛŒÚ†Û’ Ù†Û’ Ùˆ ÙˆØ§Ø± ÙˆØ§Ø³Ø·Û’ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ§Ù„Ø§ ÙˆØ§Ù„ÙˆÚº ÙˆØ§Ù„ÛŒ ÙˆØ§Ù„Û’ ÙˆØ§Û ÙˆØ¬Û ÙˆØ±Ù†Û\n",
    " ÙˆØ¹Ù„ÛŒÚ©Ù… ÙˆØºÛŒØ±Û ÙˆÙ„Û’ ÙˆÚ¯Ø±Ù†Û ÙˆÛ ÙˆÛØ§Úº ÙˆÛÛŒ ÙˆÛÛŒÚº ÙˆÛŒØ³Ø§ ÙˆÛŒØ³Û’ ÙˆÛŒÚº Ù¾Ø§Ø³\n",
    " Ù¾Ø§ÛŒØ§ Ù¾Ø± Ù¾Ø³ Ù¾Ù„ÛŒØ² Ù¾ÙˆÙ† Ù¾ÙˆÙ†Ø§ Ù¾ÙˆÙ†ÛŒ Ù¾ÙˆÙ†Û’ Ù¾Ú¾Ø§Ú¯Ù† Ù¾Ú¾Ø± Ù¾Û Ù¾ÛØ± Ù¾ÛÙ„Ø§ Ù¾ÛÙ„ÛŒ\n",
    " Ù¾ÛÙ„Û’ Ù¾ÛŒØ± Ù¾ÛŒÚ†Ú¾Û’ Ú†Ø§ÛØ¦Û’ Ú†Ø§ÛØªÛ’ Ú†Ø§ÛÛŒØ¦Û’ Ú†Ø§ÛÛ’ Ú†Ù„Ø§ Ú†Ù„Ùˆ Ú†Ù„ÛŒÚº Ú†Ù„Û’ Ú†Ù†Ø§Ú†Û Ú†Ù†Ø¯ Ú†ÙˆÙ†Ú©Û\n",
    " Ú†ÙˆÚ¯Ù†ÛŒ Ú†Ú©ÛŒ Ú†Ú©ÛŒÚº Ú†Ú©Û’ Ú†ÛØ§Ø±Ø´Ù†Ø¨Û Ú†ÛŒØª ÚˆØ§Ù„Ù†ÛŒ ÚˆØ§Ù„Ù†Û’ ÚˆØ§Ù„Û’ Ú©Ø¦Û’ Ú©Ø§ Ú©Ø§ØªÚ© Ú©Ø§Ø´ Ú©Ø¨\n",
    " Ú©Ø¨Ú¾ÛŒ Ú©Ø¯Ú¾Ø± Ú©Ø± Ú©Ø±ØªØ§ Ú©Ø±ØªÛŒ Ú©Ø±ØªÛ’ Ú©Ø±Ù… Ú©Ø±Ù†Ø§ Ú©Ø±Ù†Û’ Ú©Ø±Ùˆ Ú©Ø±ÛŒÚº Ú©Ø±Û’ Ú©Ø³\n",
    " Ú©Ø³ÛŒ Ú©Ø³Û’ Ú©Ù„ Ú©Ù… Ú©Ù† Ú©Ù†ÛÛŒÚº Ú©Ùˆ Ú©ÙˆØ¦ÛŒ Ú©ÙˆÙ† Ú©ÙˆÙ†Ø³Ø§ Ú©ÙˆÙ†Ø³Û’ Ú©Ú†Ú¾ Ú©Û Ú©ÛØ§\n",
    " Ú©ÛØ§Úº Ú©ÛÛ Ú©ÛÛŒ Ú©ÛÛŒÚº Ú©ÛÛ’ Ú©ÛŒ Ú©ÛŒØ§ Ú©ÛŒØ³Ø§ Ú©ÛŒØ³Û’ Ú©ÛŒÙˆÙ†Ú©Ø± Ú©ÛŒÙˆÙ†Ú©Û Ú©ÛŒÙˆÚº Ú©ÛŒÛ’ Ú©Û’\n",
    " Ú¯Ø¦ÛŒ Ú¯Ø¦Û’ Ú¯Ø§ Ú¯Ø±Ù…Ø§ Ú¯Ø±Ù…ÛŒ Ú¯Ù†Ø§ Ú¯Ùˆ Ú¯ÙˆÛŒØ§ Ú¯Ú¾Ù†Ù¹Ø§ Ú¯Ú¾Ù†Ù¹ÙˆÚº Ú¯Ú¾Ù†Ù¹Û’ Ú¯ÛŒ Ú¯ÛŒØ§\n",
    " ÛØ§Ø¦ÛŒÚº ÛØ§Ø¦Û’ ÛØ§Ú‘ ÛØ§Úº ÛØ± ÛØ±Ú†Ù†Ø¯ ÛØ±Ú¯Ø² ÛØ²Ø§Ø± ÛÙØªÛ ÛÙ… ÛÙ…Ø§Ø±Ø§ ÛÙ…Ø§Ø±ÛŒ ÛÙ…Ø§Ø±Û’ ÛÙ…ÛŒ\n",
    " ÛÙ…ÛŒÚº ÛÙˆ ÛÙˆØ¦ÛŒ ÛÙˆØ¦ÛŒÚº ÛÙˆØ¦Û’ ÛÙˆØ§ ÛÙˆØ¨ÛÙˆ ÛÙˆØªØ§ ÛÙˆØªÛŒ ÛÙˆØªÛŒÚº ÛÙˆØªÛ’ ÛÙˆÙ†Ø§ ÛÙˆÙ†Ú¯Û’ ÛÙˆÙ†ÛŒ\n",
    " ÛÙˆÙ†Û’ ÛÙˆÚº ÛÛŒ ÛÛŒÙ„Ùˆ ÛÛŒÚº ÛÛ’ ÛŒØ§ ÛŒØ§Øª ÛŒØ¹Ù†ÛŒ ÛŒÚ© ÛŒÛ ÛŒÛØ§Úº ÛŒÛÛŒ ÛŒÛÛŒÚº\n",
    " Ù…ÛŒÚº\n",
    "\"\"\".split())\n",
    "\n",
    "# Urdu punctuations\n",
    "URDU_PUNCTUATIONS = ['\\u200F', '\\u200f', 'Û”', 'Ù«', 'Ùª', 'ØŸ', 'ØŒ', ')', '(', '{', '}', 'â€¦', '...', 'Û”Û”Û”', '\\u002F', '\\u003F', '.']\n",
    "\n",
    "# Function to remove punctuations\n",
    "def removing_punctuations(text):\n",
    "    # Iterate over each punctuation in the list and replace it with a space\n",
    "    for punct in URDU_PUNCTUATIONS:\n",
    "        text = text.replace(punct, \" \")\n",
    "    return text\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text: str):\n",
    "    return \" \".join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# Function to clean text using regex (removes extra characters, URLs, etc.)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Remove punctuation using regex (if any left after removing URDU_PUNCTUATIONS)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.MULTILINE)\n",
    "        return text.strip()  # Remove leading/trailing spaces\n",
    "    return text  # Return as is if it's not a string\n",
    "\n",
    "# Apply the cleaning to the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    urdu_text = row['corrected_text']\n",
    "    if isinstance(urdu_text, str):\n",
    "        # Remove punctuations\n",
    "        cleaned_text = removing_punctuations(urdu_text)\n",
    "        # Remove stopwords\n",
    "        cleaned_text = remove_stopwords(cleaned_text)\n",
    "        # Further cleaning with regex\n",
    "        cleaned_text = clean_text(cleaned_text)\n",
    "        # Update the DataFrame\n",
    "        df.at[index, 'corrected_text'] = cleaned_text\n",
    "    else:\n",
    "        df.at[index, 'corrected_text'] = \"\"\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n",
    "\n",
    "# Print the first 15 rows for inspection\n",
    "df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Short Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KwnXKAWeR6KU"
   },
   "outputs": [],
   "source": [
    "# Function to check if the text has 3 or fewer words\n",
    "def has_few_words(text: str):\n",
    "    words = text.split()\n",
    "    # Check if the number of words is less than or equal to 3\n",
    "    return len(words) <= 3\n",
    "\n",
    "# Process the DataFrame and remove rows where there are 3 or fewer words\n",
    "df = df[~df['corrected_text'].apply(lambda text: has_few_words(text) if isinstance(text, str) else True)]\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "MuyWlY7rTuSc",
    "outputId": "6aeed540-9697-41dc-e388-18a179d6fb89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³ØªÛŒ Ø¢Ø¤Úº Ù…ÛŒÚº</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø§ÛŒØ³  Ø­Ø§Ù…ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø§Ù†Ø³Ø§Ù† ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ÛŒØ§Ø± Ú†Ø§Ø±Û ÙˆÛŒÙ„Ù…Ø§ Ø¢Ø±Û’ ÛÙˆÚ¯ÛŒØ§ ÛÛ’ ØªØ³Ù„ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†ÙˆÙ…ÛŒ Ù…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ØªØ³Ù„ÛŒ Ù„Ú‘Ø§Ø¦ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§Ø²ÛŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ù„Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ù„Ø§Ù„Ùˆ Ø¯ÙˆØ³ØªÙˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ Ø±ÙˆÙ¾Û’ Ú¯Ø² ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ Ø²Ø®Ù… Ú©Ú¾Ø§ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§ Ú¯Û’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø¬ Ø§Ù„Ø¹Ù…Ù„ Ayt 20 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ú©ÛŒ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ø¨Ø³ Ø¨ÛŒÙˆÛŒØ§Úº Ù¾ÛÛŒÛ’ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦Ù„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ ÛÛ’ Ø³ÛŒ Ø³ÛŒ Ú©ÛŒ Ø¨ÙˆØ±Úˆ Ú©Ùˆ Ø²Ù†Ú¯ Ù„Ú¯ ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ù¾ØªÛ Ù†ÛÛŒÚº ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø¬Ù†Ú¯ ÛÛ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ø§Ù„Ù„Û Ø¢Ù¾ Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ú©Û’ Ø³Ø§Ø¦Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Û’ Ø§ÙˆØ± Ù…Ú©...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ø§Ù„Ù„Û Ø­Ù…Ù„ÙˆÚº Ø³Ø§Ø¦Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Ø´</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  is_sarcastic  \\\n",
       "0   ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
       "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
       "5         Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
       "6       Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€           0.0   \n",
       "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...           1.0   \n",
       "9            ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚           1.0   \n",
       "10                      ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚           0.0   \n",
       "11                       Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„           0.0   \n",
       "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...           1.0   \n",
       "13   ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’           1.0   \n",
       "15  Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20...           0.0   \n",
       "16  Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ú©ÛŒ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ...           1.0   \n",
       "17  Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ ÛÛ’ Ø³ÛŒ Ø³ÛŒ Ú©ÛŒ Ø¨ÙˆØ±Úˆ Ú©Ùˆ Ø²Ù†Ú¯ Ù„Ú¯ ...           1.0   \n",
       "18  Ø§Ù„Ù„Û Ø¢Ù¾ Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ú©Û’ Ø³Ø§Ø¦Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Û’ Ø§ÙˆØ± Ù…Ú©...           0.0   \n",
       "\n",
       "                                       corrected_text  \n",
       "0                 Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’  \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³ØªÛŒ Ø¢Ø¤Úº Ù…ÛŒÚº  \n",
       "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...  \n",
       "4                      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø§ÛŒØ³  Ø­Ø§Ù…ÛŒ  \n",
       "5                             Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±  \n",
       "6                                Ø§Ù†Ø³Ø§Ù† ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±  \n",
       "8   ÛŒØ§Ø± Ú†Ø§Ø±Û ÙˆÛŒÙ„Ù…Ø§ Ø¢Ø±Û’ ÛÙˆÚ¯ÛŒØ§ ÛÛ’ ØªØ³Ù„ÛŒ ØªÛ’ Ù¾Ú©Û’ Ù†ÙˆÙ…ÛŒ Ù…...  \n",
       "9                       Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’  \n",
       "10                             ØªØ³Ù„ÛŒ Ù„Ú‘Ø§Ø¦ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§Ø²ÛŒ  \n",
       "11                             Ù„Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’  \n",
       "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ù„Ø§Ù„Ùˆ Ø¯ÙˆØ³ØªÙˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ Ø±ÙˆÙ¾Û’ Ú¯Ø² ...  \n",
       "13                     Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ Ø²Ø®Ù… Ú©Ú¾Ø§ Ú¯Û’ Ù…Ø³Ú©Ø±Ø§ Ú¯Û’  \n",
       "15  Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø¬ Ø§Ù„Ø¹Ù…Ù„ Ayt 20 ...  \n",
       "16                         Ø¨Ø³ Ø¨ÛŒÙˆÛŒØ§Úº Ù¾ÛÛŒÛ’ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦Ù„  \n",
       "17                         Ù¾ØªÛ Ù†ÛÛŒÚº ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø¬Ù†Ú¯ ÛÛ’  \n",
       "18                Ø§Ù„Ù„Û Ø­Ù…Ù„ÙˆÚº Ø³Ø§Ø¦Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Ø´  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lamatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      corrected_text\n",
      "0                Ù„ÛŒÙ†Ø§ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÙ†Ø§\n",
      "1      Ú†Ù„Ù†Ø§ Ù…ÛÙ…Ø§Ù† Ú©Ú¾Ø§ Ø³Ø±ÙˆØ³ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù† Ø¯Ø³ØªÛŒ Ø¢Ù†Ø§ Ù…ÛŒÚº\n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ù…ÛŒØ±Ø§ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ù†Ø§ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...\n",
      "3                      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø§ÛŒØ³ Ø­Ø§Ù…ÛŒ\n",
      "4                            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from LughaatNLP import LughaatNLP  # Ensure you import the necessary library\n",
    "\n",
    "# Initialize the text processing tools\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('urdu_sarcastic_dataset_cleaned.csv')\n",
    "\n",
    "# Ensure the 'urdu_text' column is string type and handle missing values\n",
    "df['urdu_text'] = df['urdu_text'].fillna('').astype(str)\n",
    "\n",
    "# Function to apply lemmatization and stemming to each row and update the 'urdu_text' column\n",
    "def lemmatize_and_stem_and_update(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Lemmatization\n",
    "        lemmatized_text = urdu_text_processing.lemmatize_sentence(row['corrected_text'])\n",
    "        # Stemming\n",
    "        stemmed_text = urdu_text_processing.urdu_stemmer(lemmatized_text)\n",
    "        # Update the 'urdu_text' column with the stemmed text\n",
    "        df.at[index, 'corrected_text'] = lemmatized_text  # You can choose to store lemmatized text or stemmed text as needed\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "lemmatize_and_stem_and_update(df)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n",
    "\n",
    "# View the updated DataFrame\n",
    "print(df[['corrected_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
      "5        Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥   \n",
      "\n",
      "                                      corrected_text  \\\n",
      "0                Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯ Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ù†ÛÛŒÚº Ú†Ø§ÛÛŒÛ’   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±ÙˆØ³ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†Ø§ Ø¯Ù†ÙˆÚº Ø¯Ø³ØªÛŒ Ø¢Ø¤Úº Ù…ÛŒÚº   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ø­Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´...   \n",
      "4                     Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø§ÛŒØ³  Ø­Ø§Ù…ÛŒ   \n",
      "5                            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø±   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0        [Ù„ÛŒÙ†Û’, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯, Ù¹Ú¾ÛŒÚ©, Ú©ÙˆØ¬ÛŒ, Ù†ÛÛŒÚº, Ú†Ø§ÛÛŒÛ’]  \n",
      "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±ÙˆØ³, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†Ø§, Ø¯Ù†ÙˆÚº, Ø¯...  \n",
      "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ø­Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§Ø¦...  \n",
      "4              [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø§ÛŒØ³, Ø­Ø§Ù…ÛŒ]  \n",
      "5                       [Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø§Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø§Ø±]  \n"
     ]
    }
   ],
   "source": [
    "import LughaatNLP\n",
    "urdu_text_processing = LughaatNLP.LughaatNLP()\n",
    "def tokenize_and_update(df):\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = urdu_text_processing.urdu_tokenize(row['corrected_text'])\n",
    "        df.at[index, 'tokenized_text'] = tokens  \n",
    "tokenize_and_update(df)\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n",
    "print(df[['urdu_text','corrected_text', 'tokenized_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with the highest TF-IDF scores:\n",
      "Ù†ÛÛŒÚº       507.874716\n",
      "Ø§Ù„Ù„Û       190.286436\n",
      "Ú¾Û’         186.081019\n",
      "Ø®Ø§Ù†        180.862081\n",
      "Ø¨Ø§Øª        162.491893\n",
      "Ø³Ù†Ø¯Ú¾       148.695508\n",
      "Ú¯Û’         143.677618\n",
      "Ù¾Ø§Ú©Ø³ØªØ§Ù†    132.623666\n",
      "Ø§Û’         129.934358\n",
      "Ù†ÙˆØ§Ø²       128.136375\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming the DataFrame 'df' is already loaded and the 'corrected_text' column exists\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corrected text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['corrected_text'])\n",
    "\n",
    "# Get the words corresponding to the features\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to store the TF-IDF scores\n",
    "tfidf_scores = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Sum the scores for each term across all documents\n",
    "sum_tfidf_scores = tfidf_scores.sum(axis=0)\n",
    "\n",
    "# Sort the scores and get the top 10 terms\n",
    "top_tfidf_terms = sum_tfidf_scores.nlargest(10)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "print(\"Top 10 words with the highest TF-IDF scores:\")\n",
    "print(top_tfidf_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram,Bigram,TriGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Unigrams:\n",
      "Ù†ÛÛŒÚº: 4551\n",
      "Ø®Ø§Ù†: 1139\n",
      "Ú¾Û’: 1126\n",
      "Ø§Ù„Ù„Û: 1116\n",
      "Ø¨Ø§Øª: 940\n",
      "Ù¾Ø§Ú©Ø³ØªØ§Ù†: 816\n",
      "Ø³Ù†Ø¯Ú¾: 812\n",
      "Ú¯Û’: 799\n",
      "Ù†ÙˆØ§Ø²: 721\n",
      "Ø¹Ù…Ø±Ø§Ù†: 699\n",
      "\n",
      "Top 10 Most Common Bigrams:\n",
      "Ø¹Ù…Ø±Ø§Ù† Ø®Ø§Ù†: 507\n",
      "Ù†ÙˆØ§Ø² Ø´Ø±ÛŒÙ: 459\n",
      "t co: 407\n",
      "Ø³Ù†Ø¯Ú¾ Ù¾ÙˆÙ„ÛŒØ³: 295\n",
      "Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ: 227\n",
      "Ú©ÛŒÙ¾Ù¹Ù† ØµÙØ¯Ø±: 184\n",
      "Ù…Ø±ÛŒÙ… Ù†ÙˆØ§Ø²: 159\n",
      "Ù† Ù„ÛŒÚ¯: 155\n",
      "Ø§Ù„Ù„Û Ø¹Ù„ÛŒÛ: 143\n",
      "Ø¬Ø²Ø§ Ø§Ù„Ù„Û: 133\n",
      "\n",
      "Top 10 Most Common Trigrams:\n",
      "ØµÙ„ÛŒ Ø§Ù„Ù„Û Ø¹Ù„ÛŒÛ: 100\n",
      "Ù¾ÛŒ ÚˆÛŒ Ø§ÛŒÙ…: 88\n",
      "ÙØ§Ù„Ùˆ ÙØ§Ù„Ùˆ Ø¨ÛŒÚ©: 71\n",
      "Ø¬Ø²Ø§ Ø§Ù„Ù„Û Ø®ÛŒØ±: 66\n",
      "sarcasm t co: 65\n",
      "Ø§Ù„Ù„Û Ø¹Ù„ÛŒÛ Ø¢Ù„Û: 61\n",
      "Ø¹Ù„ÛŒÛ Ø¢Ù„Û ÙˆØ³Ù„Ù…: 61\n",
      "Ø±ÛŒÙ¹ÙˆÛŒÙ¹ Ø±ÛŒÙ¹ÙˆÛŒÙ¹ ÙØ§Ù„Ùˆ: 59\n",
      "Ø§ÛŒØ³ Ø§ÛŒÚ† Ø§Ùˆ: 55\n",
      "Ø§Ø³ØªØºÙØ§Ø± Ø§Ù„Ù„Ù‡Û ÙˆØ§ØªÙˆØ¨: 53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming the DataFrame 'df' is already loaded and the 'corrected_text' column exists\n",
    "\n",
    "# Initialize CountVectorizer for unigram, bigram, and trigram analysis\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b')\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b')\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "# Fit and transform the corrected text for unigrams, bigrams, and trigrams\n",
    "unigram_matrix = unigram_vectorizer.fit_transform(df['corrected_text'])\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(df['corrected_text'])\n",
    "trigram_matrix = trigram_vectorizer.fit_transform(df['corrected_text'])\n",
    "\n",
    "# Get feature names for each\n",
    "unigram_features = unigram_vectorizer.get_feature_names_out()\n",
    "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "trigram_features = trigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the counts for each n-gram\n",
    "unigram_counts = unigram_matrix.sum(axis=0)\n",
    "bigram_counts = bigram_matrix.sum(axis=0)\n",
    "trigram_counts = trigram_matrix.sum(axis=0)\n",
    "\n",
    "# Create dictionaries to store n-gram frequencies\n",
    "unigram_freq = {unigram_features[i]: unigram_counts[0, i] for i in range(unigram_counts.shape[1])}\n",
    "bigram_freq = {bigram_features[i]: bigram_counts[0, i] for i in range(bigram_counts.shape[1])}\n",
    "trigram_freq = {trigram_features[i]: trigram_counts[0, i] for i in range(trigram_counts.shape[1])}\n",
    "\n",
    "# Get the top 10 most common unigrams, bigrams, and trigrams\n",
    "top_10_unigrams = Counter(unigram_freq).most_common(10)\n",
    "top_10_bigrams = Counter(bigram_freq).most_common(10)\n",
    "top_10_trigrams = Counter(trigram_freq).most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Most Common Unigrams:\")\n",
    "for unigram, freq in top_10_unigrams:\n",
    "    print(f\"{unigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Common Bigrams:\")\n",
    "for bigram, freq in top_10_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Common Trigrams:\")\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16661/16661 [00:00<00:00, 92768.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      tokenized_text\n",
      "0        [Ù„ÛŒÙ†Û’, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯, Ù¹Ú¾ÛŒÚ©, Ú©ÙˆØ¬ÛŒ, Ù†ÛÛŒÚº, Ú†Ø§ÛÛŒÛ’]\n",
      "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±ÙˆØ³, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†Ø§, Ø¯Ù†ÙˆÚº, Ø¯...\n",
      "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ø­Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§Ø¦...\n",
      "4              [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø§ÛŒØ³, Ø­Ø§Ù…ÛŒ]\n",
      "5                       [Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø§Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø§Ø±]\n",
      "Top 5 words most similar to 'Ø§Ú†Ú¾Ø§':\n",
      "Ú©Ø§ÙÛŒ: 0.8639\n",
      "Ø²Ø±Ø§: 0.8331\n",
      "Ø­Ø§Ù„: 0.8245\n",
      "Ù¾ÛŒØ§Ø±: 0.8230\n",
      "Ø¯Ú©Ú¾: 0.8211\n",
      "Word embeddings saved to 'word_embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Ensure all entries in 'lemmatize_text' column are strings and handle NaN values\n",
    "df['corrected_text'] = df['corrected_text'].fillna('').astype(str)\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenization with progress bar\n",
    "df['tokenized_text'] = df['corrected_text'].progress_apply(urdu_text_processing.urdu_tokenize)\n",
    "\n",
    "# Print the first 5 rows to check the results\n",
    "print(df[['tokenized_text']].head())\n",
    "# Train Word2Vec model on tokenized text\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Query the model for the top 5 words most similar to \"Ø§Ú†Ú¾Ø§\" (good)\n",
    "similar_words = model.wv.most_similar(\"Ø§Ú†Ú¾Ø§\", topn=5)\n",
    "\n",
    "# Display the similar words and their similarity scores\n",
    "print(\"Top 5 words most similar to 'Ø§Ú†Ú¾Ø§':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n",
    "# Train Word2Vec model on tokenized text\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Get word embeddings\n",
    "word_vectors = model.wv\n",
    "words = word_vectors.index_to_key  # List of words in the model's vocabulary\n",
    "embeddings = [word_vectors[word] for word in words]  # Corresponding embeddings\n",
    "\n",
    "# Create a DataFrame to store words and their embeddings\n",
    "embedding_df = pd.DataFrame(embeddings, index=words)\n",
    "\n",
    "# Save embeddings to CSV\n",
    "embedding_df.to_csv('word_embeddings.csv', index_label='word')\n",
    "\n",
    "# Display success message\n",
    "print(\"Word embeddings saved to 'word_embeddings.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic,SVM AND NAIVE BYS and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16661/16661 [00:00<00:00, 89975.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16661/16661 [00:00<00:00, 23372.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.7354\n",
      "Precision: 0.7380\n",
      "Recall: 0.8100\n",
      "F1 Score: 0.7723\n",
      "\n",
      "Naive Bayes Metrics:\n",
      "Accuracy: 0.7195\n",
      "Precision: 0.7246\n",
      "Recall: 0.7964\n",
      "F1 Score: 0.7588\n",
      "\n",
      "SVM Metrics:\n",
      "Accuracy: 0.7462\n",
      "Precision: 0.7346\n",
      "Recall: 0.8484\n",
      "F1 Score: 0.7874\n",
      "\n",
      "Testing Logistic Regression:\n",
      "Input: 'ÛŒÛ Ú©Ù…Ø§Ù„ Ú©Ø§ ÛÛ’ØŒ Ø¨Ø§Ù„Ú©Ù„ Ù†ÛÛŒÚº!' -> Prediction: Sarcastic\n",
      "Input: 'Ú©ØªØ§Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ú†Ú¾ÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ ØªÙˆ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’!' -> Prediction: Sarcastic\n",
      "Input: 'ÙˆØ§Û! ØªÙ… Ù†Û’ ØªÙˆ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ú©Û’ Ø¯Ù†ÛŒØ§ Ú©Ùˆ Ø­ÛŒØ±Ø§Ù† Ú©Ø± Ø¯ÛŒØ§!' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ø¨Ø§Ù„Ú©Ù„ØŒ Ø§Ø³ Ù…Ù†ØµÙˆØ¨Û’ Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©ÙˆØ¦ÛŒ Ø§Ù…ÛŒØ¯ Ù†ÛÛŒÚº!' -> Prediction: Sarcastic\n",
      "Input: 'Ú©ÛŒØ§ Ø¨Ø§Øª ÛÛ’ØŒ ØªÙ…ÛØ§Ø±ÛŒ Ø¹Ù‚Ù„ ØªÙˆ Ø¨ÛØª ÛÛŒ Ú†Ù…Ú©Ø¯Ø§Ø± ÛÛ’!' -> Prediction: Sarcastic\n",
      "Input: 'ÛŒÛ ÛÛ’ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØªØ±ÛŒÙ† Ø®ÛŒØ§Ù„ØŒ Ø¬ÛŒØ³Û’ Ú†Ø§Ø¦Û’ Ù…ÛŒÚº Ù†Ù…Ú© Ù…Ù„Ø§Ù†Ø§!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ØªÙ…ÛØ§Ø±Û’ Ø³Ø§ØªÚ¾ Ú¯ÙØªÚ¯Ùˆ Ú©Ø±Ù†Ø§ ÙˆØ§Ù‚Ø¹ÛŒ Ø§ÛŒÚ© Ø®ÙˆØ´ÛŒ Ú©ÛŒ Ø¨Ø§Øª ÛÛ’!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ Ú©ØªØ§Ø¨ Ø¨ÛØª Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ ØªØµÙˆÛŒØ± Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ù…Ø¬Ú¾Û’ ØªÙ…ÛØ§Ø±ÛŒ Ù…Ø¯Ø¯ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û”' -> Prediction: Sarcastic\n",
      "Input: 'Ø¢Ø¬ Ú©Ø§ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø§Ú†Ú¾Ø§ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ù…ÛŒÚº Ù†Û’ Ø¢Ø¬ Ø§ÛŒÚ© Ù†Ø¦ÛŒ ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ©Ú¾ÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "\n",
      "Testing Naive Bayes:\n",
      "Input: 'ÛŒÛ Ú©Ù…Ø§Ù„ Ú©Ø§ ÛÛ’ØŒ Ø¨Ø§Ù„Ú©Ù„ Ù†ÛÛŒÚº!' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ú©ØªØ§Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ú†Ú¾ÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ ØªÙˆ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÙˆØ§Û! ØªÙ… Ù†Û’ ØªÙˆ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ú©Û’ Ø¯Ù†ÛŒØ§ Ú©Ùˆ Ø­ÛŒØ±Ø§Ù† Ú©Ø± Ø¯ÛŒØ§!' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ø¨Ø§Ù„Ú©Ù„ØŒ Ø§Ø³ Ù…Ù†ØµÙˆØ¨Û’ Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©ÙˆØ¦ÛŒ Ø§Ù…ÛŒØ¯ Ù†ÛÛŒÚº!' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ú©ÛŒØ§ Ø¨Ø§Øª ÛÛ’ØŒ ØªÙ…ÛØ§Ø±ÛŒ Ø¹Ù‚Ù„ ØªÙˆ Ø¨ÛØª ÛÛŒ Ú†Ù…Ú©Ø¯Ø§Ø± ÛÛ’!' -> Prediction: Sarcastic\n",
      "Input: 'ÛŒÛ ÛÛ’ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØªØ±ÛŒÙ† Ø®ÛŒØ§Ù„ØŒ Ø¬ÛŒØ³Û’ Ú†Ø§Ø¦Û’ Ù…ÛŒÚº Ù†Ù…Ú© Ù…Ù„Ø§Ù†Ø§!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ØªÙ…ÛØ§Ø±Û’ Ø³Ø§ØªÚ¾ Ú¯ÙØªÚ¯Ùˆ Ú©Ø±Ù†Ø§ ÙˆØ§Ù‚Ø¹ÛŒ Ø§ÛŒÚ© Ø®ÙˆØ´ÛŒ Ú©ÛŒ Ø¨Ø§Øª ÛÛ’!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ Ú©ØªØ§Ø¨ Ø¨ÛØª Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ ØªØµÙˆÛŒØ± Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ù…Ø¬Ú¾Û’ ØªÙ…ÛØ§Ø±ÛŒ Ù…Ø¯Ø¯ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ø¢Ø¬ Ú©Ø§ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø§Ú†Ú¾Ø§ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ù…ÛŒÚº Ù†Û’ Ø¢Ø¬ Ø§ÛŒÚ© Ù†Ø¦ÛŒ ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ©Ú¾ÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "\n",
      "Testing SVM:\n",
      "Input: 'ÛŒÛ Ú©Ù…Ø§Ù„ Ú©Ø§ ÛÛ’ØŒ Ø¨Ø§Ù„Ú©Ù„ Ù†ÛÛŒÚº!' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ú©ØªØ§Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ú†Ú¾ÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ ØªÙˆ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’!' -> Prediction: Sarcastic\n",
      "Input: 'ÙˆØ§Û! ØªÙ… Ù†Û’ ØªÙˆ ÛŒÛ Ú©Ø§Ù… Ú©Ø± Ú©Û’ Ø¯Ù†ÛŒØ§ Ú©Ùˆ Ø­ÛŒØ±Ø§Ù† Ú©Ø± Ø¯ÛŒØ§!' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ø¨Ø§Ù„Ú©Ù„ØŒ Ø§Ø³ Ù…Ù†ØµÙˆØ¨Û’ Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©ÙˆØ¦ÛŒ Ø§Ù…ÛŒØ¯ Ù†ÛÛŒÚº!' -> Prediction: Sarcastic\n",
      "Input: 'Ú©ÛŒØ§ Ø¨Ø§Øª ÛÛ’ØŒ ØªÙ…ÛØ§Ø±ÛŒ Ø¹Ù‚Ù„ ØªÙˆ Ø¨ÛØª ÛÛŒ Ú†Ù…Ú©Ø¯Ø§Ø± ÛÛ’!' -> Prediction: Sarcastic\n",
      "Input: 'ÛŒÛ ÛÛ’ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØªØ±ÛŒÙ† Ø®ÛŒØ§Ù„ØŒ Ø¬ÛŒØ³Û’ Ú†Ø§Ø¦Û’ Ù…ÛŒÚº Ù†Ù…Ú© Ù…Ù„Ø§Ù†Ø§!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ØªÙ…ÛØ§Ø±Û’ Ø³Ø§ØªÚ¾ Ú¯ÙØªÚ¯Ùˆ Ú©Ø±Ù†Ø§ ÙˆØ§Ù‚Ø¹ÛŒ Ø§ÛŒÚ© Ø®ÙˆØ´ÛŒ Ú©ÛŒ Ø¨Ø§Øª ÛÛ’!' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ Ú©ØªØ§Ø¨ Ø¨ÛØª Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ ØªØµÙˆÛŒØ± Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ù…Ø¬Ú¾Û’ ØªÙ…ÛØ§Ø±ÛŒ Ù…Ø¯Ø¯ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û”' -> Prediction: Sarcastic\n",
      "Input: 'Ø¢Ø¬ Ú©Ø§ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø§Ú†Ú¾Ø§ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n",
      "Input: 'Ù…ÛŒÚº Ù†Û’ Ø¢Ø¬ Ø§ÛŒÚ© Ù†Ø¦ÛŒ ØªØ±Ú©ÛŒØ¨ Ø³ÛŒÚ©Ú¾ÛŒ ÛÛ’Û”' -> Prediction: Non-Sarcastic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB  # Use Gaussian Naive Bayes\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure all entries in 'corrected_text' are strings and handle NaN values\n",
    "df['corrected_text'] = df['corrected_text'].fillna('').astype(str)\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenization with progress bar\n",
    "df['tokenized_text'] = df['corrected_text'].progress_apply(urdu_text_processing.urdu_tokenize)\n",
    "\n",
    "# Train Word2Vec model on tokenized text\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Function to create feature vectors by averaging word embeddings\n",
    "def get_feature_vector(tokens, model):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[valid_tokens], axis=0)\n",
    "\n",
    "# Create feature vectors for the entire dataset\n",
    "df['feature_vectors'] = df['tokenized_text'].progress_apply(lambda x: get_feature_vector(x, model))\n",
    "\n",
    "# Create a DataFrame for features and labels\n",
    "X = np.array(list(df['feature_vectors']))\n",
    "y = df['is_sarcastic'].values  # Ensure 'is_sarcastic' is your actual label column\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(classifier, X_train, y_train, X_test, y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_classifier = LogisticRegression(max_iter=1000)\n",
    "logistic_metrics = evaluate_model(logistic_classifier, X_train, y_train, X_test, y_test)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {logistic_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {logistic_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {logistic_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {logistic_metrics[3]:.4f}\\n\")\n",
    "\n",
    "# Naive Bayes\n",
    "naive_bayes_classifier = GaussianNB()  # Use Gaussian Naive Bayes\n",
    "naive_bayes_metrics = evaluate_model(naive_bayes_classifier, X_train, y_train, X_test, y_test)\n",
    "print(\"Naive Bayes Metrics:\")\n",
    "print(f\"Accuracy: {naive_bayes_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {naive_bayes_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {naive_bayes_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {naive_bayes_metrics[3]:.4f}\\n\")\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_classifier = SVC()\n",
    "svm_metrics = evaluate_model(svm_classifier, X_train, y_train, X_test, y_test)\n",
    "print(\"SVM Metrics:\")\n",
    "print(f\"Accuracy: {svm_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {svm_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {svm_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {svm_metrics[3]:.4f}\\n\")\n",
    "\n",
    "# Testing the models with sample inputs\n",
    "def test_model(classifier, samples):\n",
    "    for text in samples:\n",
    "        tokens = urdu_text_processing.urdu_tokenize(text)\n",
    "        feature_vector = get_feature_vector(tokens, model).reshape(1, -1)  # Reshape for prediction\n",
    "        prediction = classifier.predict(feature_vector)\n",
    "        sentiment = \"Sarcastic\" if prediction[0] == 1 else \"Non-Sarcastic\"\n",
    "        print(f\"Input: '{text}' -> Prediction: {sentiment}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Logistic Regression:\n",
      "Input: 'Ú©ØªÙ†Û’ Ø§Ú†Ú¾Û’ ÛÛŒÚº ÛŒÛ Ù„ÙˆÚ¯ Ø¬Ùˆ ÛØ± ÙˆÙ‚Øª Ù…ÛŒØ±Ø§ Ù…Ø°Ø§Ù‚ Ø§Ú‘Ø§ØªÛ’ ÛÛŒÚº :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ´Ú¯ÙˆØ§Ø± ÛÛ’Û” :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ Ú©Ú¾Ø§Ù†Ø§ Ø¨ÛØª Ù…Ø²ÛŒØ¯Ø§Ø± ÛÛ’ :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØª Ù…Ø²Û Ø¢ÛŒØ§ØŒ ØªÙ… Ù†Û’ Ù…Ø²Û’ Ú©Ø§ Ø¨ÛŒÚ‘Ø§ ØºØ±Ù‚ Ú©Ø± Ø¯ÛŒØ§ - :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'Ú©ÛŒØ§ Ø²Ø¨Ø±Ø¯Ø³Øª ÙÛŒØµÙ„Û ØªÚ¾Ø§ØŒ Ø³Ø¨ Ú©Ú†Ú¾ Ø¨Ø±Ø¨Ø§Ø¯ ÛÙˆ Ú¯ÛŒØ§ :Sentence' -> Prediction: Sarcastic\n",
      "\n",
      "Testing Naive Bayes:\n",
      "Input: 'Ú©ØªÙ†Û’ Ø§Ú†Ú¾Û’ ÛÛŒÚº ÛŒÛ Ù„ÙˆÚ¯ Ø¬Ùˆ ÛØ± ÙˆÙ‚Øª Ù…ÛŒØ±Ø§ Ù…Ø°Ø§Ù‚ Ø§Ú‘Ø§ØªÛ’ ÛÛŒÚº :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ´Ú¯ÙˆØ§Ø± ÛÛ’Û” :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ Ú©Ú¾Ø§Ù†Ø§ Ø¨ÛØª Ù…Ø²ÛŒØ¯Ø§Ø± ÛÛ’ :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØª Ù…Ø²Û Ø¢ÛŒØ§ØŒ ØªÙ… Ù†Û’ Ù…Ø²Û’ Ú©Ø§ Ø¨ÛŒÚ‘Ø§ ØºØ±Ù‚ Ú©Ø± Ø¯ÛŒØ§ - :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'Ú©ÛŒØ§ Ø²Ø¨Ø±Ø¯Ø³Øª ÙÛŒØµÙ„Û ØªÚ¾Ø§ØŒ Ø³Ø¨ Ú©Ú†Ú¾ Ø¨Ø±Ø¨Ø§Ø¯ ÛÙˆ Ú¯ÛŒØ§ :Sentence' -> Prediction: Sarcastic\n",
      "\n",
      "Testing SVM:\n",
      "Input: 'Ú©ØªÙ†Û’ Ø§Ú†Ú¾Û’ ÛÛŒÚº ÛŒÛ Ù„ÙˆÚ¯ Ø¬Ùˆ ÛØ± ÙˆÙ‚Øª Ù…ÛŒØ±Ø§ Ù…Ø°Ø§Ù‚ Ø§Ú‘Ø§ØªÛ’ ÛÛŒÚº :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ´Ú¯ÙˆØ§Ø± ÛÛ’Û” :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÛŒÛ Ú©Ú¾Ø§Ù†Ø§ Ø¨ÛØª Ù…Ø²ÛŒØ¯Ø§Ø± ÛÛ’ :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØª Ù…Ø²Û Ø¢ÛŒØ§ØŒ ØªÙ… Ù†Û’ Ù…Ø²Û’ Ú©Ø§ Ø¨ÛŒÚ‘Ø§ ØºØ±Ù‚ Ú©Ø± Ø¯ÛŒØ§ - :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'Ú©ÛŒØ§ Ø²Ø¨Ø±Ø¯Ø³Øª ÙÛŒØµÙ„Û ØªÚ¾Ø§ØŒ Ø³Ø¨ Ú©Ú†Ú¾ Ø¨Ø±Ø¨Ø§Ø¯ ÛÙˆ Ú¯ÛŒØ§ :Sentence' -> Prediction: Sarcastic\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"Ú©ØªÙ†Û’ Ø§Ú†Ú¾Û’ ÛÛŒÚº ÛŒÛ Ù„ÙˆÚ¯ Ø¬Ùˆ ÛØ± ÙˆÙ‚Øª Ù…ÛŒØ±Ø§ Ù…Ø°Ø§Ù‚ Ø§Ú‘Ø§ØªÛ’ ÛÛŒÚº :Sentence\",  # Sarcastic\n",
    "    \"Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ´Ú¯ÙˆØ§Ø± ÛÛ’Û” :Sentence\",  # Non-Sarcastic\n",
    "    \"ÛŒÛ Ú©Ú¾Ø§Ù†Ø§ Ø¨ÛØª Ù…Ø²ÛŒØ¯Ø§Ø± ÛÛ’ :Sentence\",  # Non-Sarcastic\n",
    "    \"ÙˆØ§Ù‚Ø¹ÛŒ Ø¨ÛØª Ù…Ø²Û Ø¢ÛŒØ§ØŒ ØªÙ… Ù†Û’ Ù…Ø²Û’ Ú©Ø§ Ø¨ÛŒÚ‘Ø§ ØºØ±Ù‚ Ú©Ø± Ø¯ÛŒØ§ - :Sentence\",  # Sarcastic\n",
    "    \"Ú©ÛŒØ§ Ø²Ø¨Ø±Ø¯Ø³Øª ÙÛŒØµÙ„Û ØªÚ¾Ø§ØŒ Ø³Ø¨ Ú©Ú†Ú¾ Ø¨Ø±Ø¨Ø§Ø¯ ÛÙˆ Ú¯ÛŒØ§ :Sentence\"  # Sarcastic\n",
    "]\n",
    "print(\"Testing Logistic Regression:\")\n",
    "test_model(logistic_classifier, sample_texts)\n",
    "print(\"\\nTesting Naive Bayes:\")\n",
    "test_model(naive_bayes_classifier, sample_texts)\n",
    "print(\"\\nTesting SVM:\")\n",
    "test_model(svm_classifier, sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
