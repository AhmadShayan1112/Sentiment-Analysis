{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "koI9bHBwIcaW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\ahmed\\urdu_sarcastic_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrUFRGcZCbl-",
    "outputId": "fdc66fe0-5675-4660-d82e-63cc13cf21ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lughaatNLP in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (1.13.1)\n",
      "Requirement already satisfied: gtts in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (2.5.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (3.10.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from lughaatNLP) (0.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->lughaatNLP) (2.32.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->lughaatNLP) (8.1.7)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from python-Levenshtein->lughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from Levenshtein==0.26.0->python-Levenshtein->lughaatNLP) (3.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->lughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->lughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from SpeechRecognition->lughaatNLP) (4.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow->lughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts->lughaatNLP) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->lughaatNLP) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install lughaatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yYqyuaBpV-yD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import LughaatNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "rsgKmdzcWAXn",
    "outputId": "872b2a37-9c94-416c-91e0-851692c360ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20060/20060 [32:55<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  \\\n",
      "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
      "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
      "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
      "3                                       نہیں پائین 😎   \n",
      "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
      "\n",
      "                                      corrected_text  \n",
      "0  🤣😂😂 ہو لینے دے میری شادی فساد ٹھیک ہے کوجی نہی...  \n",
      "1  چل مہمانوں میں کھانا سروس کر چڑیل چاچا دنوں دس...  \n",
      "2  کامران خان آپکی دن بحریہ زمہ داری لگائی گئی اپ...  \n",
      "3                                      نہیں پائینل 😎  \n",
      "4  `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی ...  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df = pd.read_csv('urdu_sarcastic_dataset.csv')\n",
    "df['urdu_text'] = df['urdu_text'].fillna('').astype(str)\n",
    "spell_checker = LughaatNLP.LughaatNLP()\n",
    "def process_sentence(sentence):\n",
    "    return spell_checker.corrected_sentence_spelling(sentence, 60)\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "df['corrected_text'] = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_sentence)(sentence) for sentence in tqdm(df['urdu_text'])\n",
    ")\n",
    "print(df[['urdu_text', 'corrected_text']].head())\n",
    "df.to_csv('urdu_sarcastic_dataset_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcYYDg8PCFc8",
    "outputId": "0de573e6-bc5d-4f93-b590-ca3e8c794314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: LughaatNLP in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (1.13.1)\n",
      "Requirement already satisfied: gtts in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (2.5.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (3.10.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from LughaatNLP) (0.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->LughaatNLP) (2.32.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gtts->LughaatNLP) (8.1.7)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from python-Levenshtein->LughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from Levenshtein==0.26.0->python-Levenshtein->LughaatNLP) (3.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->LughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from scikit-learn->LughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from SpeechRecognition->LughaatNLP) (4.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow->LughaatNLP) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts->LughaatNLP) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts->LughaatNLP) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow->LughaatNLP) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install LughaatNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NAN Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PlVxJMqWI6qh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('urdu_sarcastic_dataset_corrected.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df = df.dropna(subset=['urdu_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "oLlk9NL0TRHp",
    "outputId": "e1e3e78d-33ea-4acb-dbb3-e2ae5cb37d0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فساد ٹھیک ہے کوجی نہی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>چل مہمانوں میں کھانا سروس کر چڑیل چاچا دنوں دس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>کامران خان آپکی دن بحریہ زمہ داری لگائی گئی اپ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>0.0</td>\n",
       "      <td>نہیں پائینل 😎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>0.0</td>\n",
       "      <td>انسان کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>0.0</td>\n",
       "      <td>حامی میر صاحب ویلڈن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>یار چارہ ویلما ہونا ہے اس آرے لگا ہوگیا ہے😂😂 ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>1.0</td>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی ساڈی کی 😂😂😂</td>\n",
       "      <td>0.0</td>\n",
       "      <td>تسلی لڑائی کروانی سازی کی 😂😂😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>پائن دوبارہ فالو کرئیے..؟؟😆🙄</td>\n",
       "      <td>0.0</td>\n",
       "      <td>لائن دوبارہ فالو کرئیے..؟؟😆🙄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>کتنی مہنگائی ہے لالو دوستو روپے درجن کدو روپے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے</td>\n",
       "      <td>1.0</td>\n",
       "      <td>😍عشق جب تم کو راس آۓ گا زخم کھا گے مسکرا گے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>چونا ایسا ہی ہوتا 😂</td>\n",
       "      <td>0.0</td>\n",
       "      <td>چونا ایسا ہی ہوتا 😂</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  is_sarcastic  \\\n",
       "0   🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...           1.0   \n",
       "1   چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0   \n",
       "2   کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0   \n",
       "3                                        نہیں پائین 😎           0.0   \n",
       "4    `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...           1.0   \n",
       "5         قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥           1.0   \n",
       "6       انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀           0.0   \n",
       "7                               حامد میر صاحب ویلڈن👏😊           0.0   \n",
       "8   یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...           1.0   \n",
       "9            یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂           1.0   \n",
       "10                      تسی لڑاںٔی کروانی ساڈی کی 😂😂😂           0.0   \n",
       "11                       پائن دوبارہ فالو کرئیے..؟؟😆🙄           0.0   \n",
       "12  کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے ...           1.0   \n",
       "13   😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے           1.0   \n",
       "14                                چونا ایسا ہی ہوتا 😂           0.0   \n",
       "\n",
       "                                       corrected_text  \n",
       "0   🤣😂😂 ہو لینے دے میری شادی فساد ٹھیک ہے کوجی نہی...  \n",
       "1   چل مہمانوں میں کھانا سروس کر چڑیل چاچا دنوں دس...  \n",
       "2   کامران خان آپکی دن بحریہ زمہ داری لگائی گئی اپ...  \n",
       "3                                       نہیں پائینل 😎  \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی ...  \n",
       "5         قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥  \n",
       "6       انسان کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀  \n",
       "7                                 حامی میر صاحب ویلڈن  \n",
       "8   یار چارہ ویلما ہونا ہے اس آرے لگا ہوگیا ہے😂😂 ت...  \n",
       "9            یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂  \n",
       "10                      تسلی لڑائی کروانی سازی کی 😂😂😂  \n",
       "11                       لائن دوبارہ فالو کرئیے..؟؟😆🙄  \n",
       "12  کتنی مہنگائی ہے لالو دوستو روپے درجن کدو روپے ...  \n",
       "13        😍عشق جب تم کو راس آۓ گا زخم کھا گے مسکرا گے  \n",
       "14                                چونا ایسا ہی ہوتا 😂  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Puntion , Stop words , Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "oKG6m7i0I-cF",
    "outputId": "6ea82bac-83b8-4d55-9582-478ae228e9c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>لینے شادی فساد ٹھیک کوجی نہیں چاہیے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>چل مہمانوں کھانا سروس چڑیل چاچا دنوں دستی آؤں میں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>کامران خان آپکی دن بحریہ زمہ داری لگائی اپوزیش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہیں پائین 😎</td>\n",
       "      <td>0.0</td>\n",
       "      <td>نہیں پائینل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>مراد علی شاہ بھیس ڈی ایس  حامی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>قابل اعتبار قاتل اعتبار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>0.0</td>\n",
       "      <td>انسان تھکا سوچوں سفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>حامد میر صاحب ویلڈن👏😊</td>\n",
       "      <td>0.0</td>\n",
       "      <td>حامی میر ویلڈن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>یار چارہ ویلما آرے ہوگیا ہے تسلی تے پکے نومی م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>1.0</td>\n",
       "      <td>سمجھتے سارا پاکستان بیوقوف ھے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی ساڈی کی 😂😂😂</td>\n",
       "      <td>0.0</td>\n",
       "      <td>تسلی لڑائی کروانی سازی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>پائن دوبارہ فالو کرئیے..؟؟😆🙄</td>\n",
       "      <td>0.0</td>\n",
       "      <td>لائن دوبارہ فالو کرئیے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>کتنی مہنگائی لالو دوستو روپے درجن کدو روپے گز ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے</td>\n",
       "      <td>1.0</td>\n",
       "      <td>عشق راس آۓ زخم کھا گے مسکرا گے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>چونا ایسا ہی ہوتا 😂</td>\n",
       "      <td>0.0</td>\n",
       "      <td>چونا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  is_sarcastic  \\\n",
       "0   🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...           1.0   \n",
       "1   چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0   \n",
       "2   کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0   \n",
       "3                                        نہیں پائین 😎           0.0   \n",
       "4    `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...           1.0   \n",
       "5         قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥           1.0   \n",
       "6       انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀           0.0   \n",
       "7                               حامد میر صاحب ویلڈن👏😊           0.0   \n",
       "8   یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...           1.0   \n",
       "9            یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂           1.0   \n",
       "10                      تسی لڑاںٔی کروانی ساڈی کی 😂😂😂           0.0   \n",
       "11                       پائن دوبارہ فالو کرئیے..؟؟😆🙄           0.0   \n",
       "12  کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے ...           1.0   \n",
       "13   😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے           1.0   \n",
       "14                                چونا ایسا ہی ہوتا 😂           0.0   \n",
       "\n",
       "                                       corrected_text  \n",
       "0                 لینے شادی فساد ٹھیک کوجی نہیں چاہیے  \n",
       "1   چل مہمانوں کھانا سروس چڑیل چاچا دنوں دستی آؤں میں  \n",
       "2   کامران خان آپکی دن بحریہ زمہ داری لگائی اپوزیش...  \n",
       "3                                         نہیں پائینل  \n",
       "4                      مراد علی شاہ بھیس ڈی ایس  حامی  \n",
       "5                             قابل اعتبار قاتل اعتبار  \n",
       "6                                انسان تھکا سوچوں سفر  \n",
       "7                                      حامی میر ویلڈن  \n",
       "8   یار چارہ ویلما آرے ہوگیا ہے تسلی تے پکے نومی م...  \n",
       "9                       سمجھتے سارا پاکستان بیوقوف ھے  \n",
       "10                             تسلی لڑائی کروانی سازی  \n",
       "11                             لائن دوبارہ فالو کرئیے  \n",
       "12  کتنی مہنگائی لالو دوستو روپے درجن کدو روپے گز ...  \n",
       "13                     عشق راس آۓ زخم کھا گے مسکرا گے  \n",
       "14                                               چونا  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "stop_words = frozenset(\"\"\"\n",
    "آ آئی آئیں آئے آتا آتی آتے آداب آدھ آدھا آدھی آدھے آس\n",
    " آمدید آنا آنسہ آنی آنے آپ آگے آہ آہا آیا اب ابھی ابے\n",
    " اتوار ارب اربویں ارے اس اسکا اسکی اسکے اسی اسے اف افوہ الاول البتہ\n",
    " الثانی الحرام السلام الف المکرم ان اندر انکا انکی انکے انہوں انہی انہیں\n",
    " اوئے اور اوپر اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکبر اکثر اگر اگرچہ\n",
    " اگست اہاہا ایسا ایسی ایسے ایک بائیں بار بارے بالکل باوجود باہر بج بجے\n",
    " بخیر برسات بشرطیکہ بعض بغیر بلکہ بن بنا بناؤ بند بڑی بھر بھریں\n",
    " بھی بہار بہت بہتر بیگم تاکہ تاہم تب تجھ تجھی تجھے ترا تری\n",
    " تلک تم تمام تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے\n",
    " تہائی تیرا تیری تیرے تین جا جاؤ جائیں جائے جاتا جاتی جاتے جانی جانے\n",
    " جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی جیسا\n",
    " جیسوں جیسی جیسے جیٹھ حالانکہ حالاں حصہ حضرت خاطر خالی خدا خزاں خواہ خوب\n",
    " خود دائیں درمیان دریں دو دوران دوسرا دوسروں دوسری دوشنبہ دوں دکھائیں دگنا دی\n",
    " دیئے دیا دیتا دیتی دیتے دیر دینا دینی دینے دیکھو دیں دیے دے ذریعے\n",
    " رکھا رکھتا رکھتی رکھتے رکھنا رکھنی رکھنے رکھو رکھی رکھے رہ رہا رہتا\n",
    " رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی\n",
    " سراسر سلام سمیت سوا سوائے سکا سکتا سکتے سہ سہی سی سے شام شاید\n",
    " شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور علاوہ عین فروری فقط فلاں\n",
    " فی قبل قطا لائی لائے لاتا لاتی لاتے لانا لانی لایا لو لوجی لوگوں\n",
    " لگ لگا لگتا لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن\n",
    " لیں لیے لے ماسوا مت مجھ مجھی مجھے محترم محترمی محض مرا مرحبا\n",
    " مری مرے مزید مس مسز مسٹر مطابق مطلق مل منٹ منٹوں مکرمی مگر\n",
    " مگھر مہربانی میرا میروں میری میرے میں نا نزدیک نما نو نومبر نہ\n",
    " نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ\n",
    " وعلیکم وغیرہ ولے وگرنہ وہ وہاں وہی وہیں ویسا ویسے ویں پاس\n",
    " پایا پر پس پلیز پون پونا پونی پونے پھاگن پھر پہ پہر پہلا پہلی\n",
    " پہلے پیر پیچھے چاہئے چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ\n",
    " چوگنی چکی چکیں چکے چہارشنبہ چیت ڈالنی ڈالنے ڈالے کئے کا کاتک کاش کب\n",
    " کبھی کدھر کر کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس\n",
    " کسی کسے کل کم کن کنہیں کو کوئی کون کونسا کونسے کچھ کہ کہا\n",
    " کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے کے\n",
    " گئی گئے گا گرما گرمی گنا گو گویا گھنٹا گھنٹوں گھنٹے گی گیا\n",
    " ہائیں ہائے ہاڑ ہاں ہر ہرچند ہرگز ہزار ہفتہ ہم ہمارا ہماری ہمارے ہمی\n",
    " ہمیں ہو ہوئی ہوئیں ہوئے ہوا ہوبہو ہوتا ہوتی ہوتیں ہوتے ہونا ہونگے ہونی\n",
    " ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\n",
    " میں\n",
    "\"\"\".split())\n",
    "\n",
    "# Urdu punctuations\n",
    "URDU_PUNCTUATIONS = ['\\u200F', '\\u200f', '۔', '٫', '٪', '؟', '،', ')', '(', '{', '}', '…', '...', '۔۔۔', '\\u002F', '\\u003F', '.']\n",
    "\n",
    "# Function to remove punctuations\n",
    "def removing_punctuations(text):\n",
    "    # Iterate over each punctuation in the list and replace it with a space\n",
    "    for punct in URDU_PUNCTUATIONS:\n",
    "        text = text.replace(punct, \" \")\n",
    "    return text\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text: str):\n",
    "    return \" \".join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# Function to clean text using regex (removes extra characters, URLs, etc.)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Remove punctuation using regex (if any left after removing URDU_PUNCTUATIONS)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.MULTILINE)\n",
    "        return text.strip()  # Remove leading/trailing spaces\n",
    "    return text  # Return as is if it's not a string\n",
    "\n",
    "# Apply the cleaning to the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    urdu_text = row['corrected_text']\n",
    "    if isinstance(urdu_text, str):\n",
    "        # Remove punctuations\n",
    "        cleaned_text = removing_punctuations(urdu_text)\n",
    "        # Remove stopwords\n",
    "        cleaned_text = remove_stopwords(cleaned_text)\n",
    "        # Further cleaning with regex\n",
    "        cleaned_text = clean_text(cleaned_text)\n",
    "        # Update the DataFrame\n",
    "        df.at[index, 'corrected_text'] = cleaned_text\n",
    "    else:\n",
    "        df.at[index, 'corrected_text'] = \"\"\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n",
    "\n",
    "# Print the first 15 rows for inspection\n",
    "df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Short Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KwnXKAWeR6KU"
   },
   "outputs": [],
   "source": [
    "# Function to check if the text has 3 or fewer words\n",
    "def has_few_words(text: str):\n",
    "    words = text.split()\n",
    "    # Check if the number of words is less than or equal to 3\n",
    "    return len(words) <= 3\n",
    "\n",
    "# Process the DataFrame and remove rows where there are 3 or fewer words\n",
    "df = df[~df['corrected_text'].apply(lambda text: has_few_words(text) if isinstance(text, str) else True)]\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "MuyWlY7rTuSc",
    "outputId": "6aeed540-9697-41dc-e388-18a179d6fb89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>corrected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>لینے شادی فساد ٹھیک کوجی نہیں چاہیے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>چل مہمانوں کھانا سروس چڑیل چاچا دنوں دستی آؤں میں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>کامران خان آپکی دن بحریہ زمہ داری لگائی اپوزیش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>مراد علی شاہ بھیس ڈی ایس  حامی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>1.0</td>\n",
       "      <td>قابل اعتبار قاتل اعتبار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀</td>\n",
       "      <td>0.0</td>\n",
       "      <td>انسان تھکا سوچوں سفر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>یار چارہ ویلما آرے ہوگیا ہے تسلی تے پکے نومی م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂</td>\n",
       "      <td>1.0</td>\n",
       "      <td>سمجھتے سارا پاکستان بیوقوف ھے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>تسی لڑاںٔی کروانی ساڈی کی 😂😂😂</td>\n",
       "      <td>0.0</td>\n",
       "      <td>تسلی لڑائی کروانی سازی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>پائن دوبارہ فالو کرئیے..؟؟😆🙄</td>\n",
       "      <td>0.0</td>\n",
       "      <td>لائن دوبارہ فالو کرئیے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>کتنی مہنگائی لالو دوستو روپے درجن کدو روپے گز ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے</td>\n",
       "      <td>1.0</td>\n",
       "      <td>عشق راس آۓ زخم کھا گے مسکرا گے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>خاتم_النبیین_محمدﷺ Surat 73 سورة المزمل Ayt 20...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>خاتم_النبیین_محمدﷺ Surat 73 سورج العمل Ayt 20 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>اب بس بھی کرو بیچارے کی پہلے ہی دو بیویاں ہیے ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>بس بیویاں پہیے تیسری ٹرائل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>پتہ نہیں کیا ہورہا ہے سی سی کی بورڈ کو زنگ لگ ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>پتہ نہیں ہورہا بورڈ جنگ ہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>اللہ آپ کو اپنی رحمتوں کے سائے میں رکھے اور مک...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>اللہ حملوں سائے مکمل صحت یاب فرمائش</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            urdu_text  is_sarcastic  \\\n",
       "0   🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...           1.0   \n",
       "1   چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...           1.0   \n",
       "2   کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...           0.0   \n",
       "4    `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...           1.0   \n",
       "5         قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥           1.0   \n",
       "6       انساں کو تھکا دیتا ہے سوچوں کا سفر بھی ... 🍁🥀           0.0   \n",
       "8   یار وچارہ ویلا ہوندا ہے اس آرے لگا ہویا ہے😂😂 ت...           1.0   \n",
       "9            یہ سمجھتے ہیں سارا پاکستان بیوقوف ھے 😂😂😂           1.0   \n",
       "10                      تسی لڑاںٔی کروانی ساڈی کی 😂😂😂           0.0   \n",
       "11                       پائن دوبارہ فالو کرئیے..؟؟😆🙄           0.0   \n",
       "12  کتنی مہنگائی ہے الو دوسو روپے درجن کدو 80روپے ...           1.0   \n",
       "13   😍عشق جب تم کو راس آۓ گا 💔زخم کھاٶ گے 😊مُسکراٶ گے           1.0   \n",
       "15  خاتم_النبیین_محمدﷺ Surat 73 سورة المزمل Ayt 20...           0.0   \n",
       "16  اب بس بھی کرو بیچارے کی پہلے ہی دو بیویاں ہیے ...           1.0   \n",
       "17  پتہ نہیں کیا ہورہا ہے سی سی کی بورڈ کو زنگ لگ ...           1.0   \n",
       "18  اللہ آپ کو اپنی رحمتوں کے سائے میں رکھے اور مک...           0.0   \n",
       "\n",
       "                                       corrected_text  \n",
       "0                 لینے شادی فساد ٹھیک کوجی نہیں چاہیے  \n",
       "1   چل مہمانوں کھانا سروس چڑیل چاچا دنوں دستی آؤں میں  \n",
       "2   کامران خان آپکی دن بحریہ زمہ داری لگائی اپوزیش...  \n",
       "4                      مراد علی شاہ بھیس ڈی ایس  حامی  \n",
       "5                             قابل اعتبار قاتل اعتبار  \n",
       "6                                انسان تھکا سوچوں سفر  \n",
       "8   یار چارہ ویلما آرے ہوگیا ہے تسلی تے پکے نومی م...  \n",
       "9                       سمجھتے سارا پاکستان بیوقوف ھے  \n",
       "10                             تسلی لڑائی کروانی سازی  \n",
       "11                             لائن دوبارہ فالو کرئیے  \n",
       "12  کتنی مہنگائی لالو دوستو روپے درجن کدو روپے گز ...  \n",
       "13                     عشق راس آۓ زخم کھا گے مسکرا گے  \n",
       "15  خاتم_النبیین_محمدﷺ Surat 73 سورج العمل Ayt 20 ...  \n",
       "16                         بس بیویاں پہیے تیسری ٹرائل  \n",
       "17                         پتہ نہیں ہورہا بورڈ جنگ ہے  \n",
       "18                اللہ حملوں سائے مکمل صحت یاب فرمائش  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lamatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      corrected_text\n",
      "0                لینا شادی فساد ٹھیک کوجی نہیں چاہنا\n",
      "1      چلنا مہمان کھا سروس چڑیل چاچا دن دستی آنا میں\n",
      "2  کامران خان میرا دن بحریہ زمہ داری لگنا اپوزیشن...\n",
      "3                      مراد علی شاہ بھیس ڈی ایس حامی\n",
      "4                            قابل اعتبار قاتل اعتبار\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from LughaatNLP import LughaatNLP  # Ensure you import the necessary library\n",
    "\n",
    "# Initialize the text processing tools\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('urdu_sarcastic_dataset_cleaned.csv')\n",
    "\n",
    "# Ensure the 'urdu_text' column is string type and handle missing values\n",
    "df['urdu_text'] = df['urdu_text'].fillna('').astype(str)\n",
    "\n",
    "# Function to apply lemmatization and stemming to each row and update the 'urdu_text' column\n",
    "def lemmatize_and_stem_and_update(df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Lemmatization\n",
    "        lemmatized_text = urdu_text_processing.lemmatize_sentence(row['corrected_text'])\n",
    "        # Stemming\n",
    "        stemmed_text = urdu_text_processing.urdu_stemmer(lemmatized_text)\n",
    "        # Update the 'urdu_text' column with the stemmed text\n",
    "        df.at[index, 'corrected_text'] = lemmatized_text  # You can choose to store lemmatized text or stemmed text as needed\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "lemmatize_and_stem_and_update(df)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n",
    "\n",
    "# View the updated DataFrame\n",
    "print(df[['corrected_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           urdu_text  \\\n",
      "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
      "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
      "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
      "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
      "5        قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥   \n",
      "\n",
      "                                      corrected_text  \\\n",
      "0                لینے شادی فساد ٹھیک کوجی نہیں چاہیے   \n",
      "1  چل مہمانوں کھانا سروس چڑیل چاچا دنوں دستی آؤں میں   \n",
      "2  کامران خان آپکی دن بحریہ زمہ داری لگائی اپوزیش...   \n",
      "4                     مراد علی شاہ بھیس ڈی ایس  حامی   \n",
      "5                            قابل اعتبار قاتل اعتبار   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0        [لینے, شادی, فساد, ٹھیک, کوجی, نہیں, چاہیے]  \n",
      "1  [چل, مہمانوں, کھانا, سروس, چڑیل, چاچا, دنوں, د...  \n",
      "2  [کامران, خان, آپکی, دن, بحریہ, زمہ, داری, لگائ...  \n",
      "4              [مراد, علی, شاہ, بھیس, ڈی, ایس, حامی]  \n",
      "5                       [قابل, اعتبار, قاتل, اعتبار]  \n"
     ]
    }
   ],
   "source": [
    "import LughaatNLP\n",
    "urdu_text_processing = LughaatNLP.LughaatNLP()\n",
    "def tokenize_and_update(df):\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = urdu_text_processing.urdu_tokenize(row['corrected_text'])\n",
    "        df.at[index, 'tokenized_text'] = tokens  \n",
    "tokenize_and_update(df)\n",
    "df.to_csv('urdu_sarcastic_dataset_cleaned.csv', index=False)\n",
    "print(df[['urdu_text','corrected_text', 'tokenized_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with the highest TF-IDF scores:\n",
      "نہیں       507.874716\n",
      "اللہ       190.286436\n",
      "ھے         186.081019\n",
      "خان        180.862081\n",
      "بات        162.491893\n",
      "سندھ       148.695508\n",
      "گے         143.677618\n",
      "پاکستان    132.623666\n",
      "اے         129.934358\n",
      "نواز       128.136375\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming the DataFrame 'df' is already loaded and the 'corrected_text' column exists\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corrected text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['corrected_text'])\n",
    "\n",
    "# Get the words corresponding to the features\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to store the TF-IDF scores\n",
    "tfidf_scores = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Sum the scores for each term across all documents\n",
    "sum_tfidf_scores = tfidf_scores.sum(axis=0)\n",
    "\n",
    "# Sort the scores and get the top 10 terms\n",
    "top_tfidf_terms = sum_tfidf_scores.nlargest(10)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "print(\"Top 10 words with the highest TF-IDF scores:\")\n",
    "print(top_tfidf_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram,Bigram,TriGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Unigrams:\n",
      "نہیں: 4551\n",
      "خان: 1139\n",
      "ھے: 1126\n",
      "اللہ: 1116\n",
      "بات: 940\n",
      "پاکستان: 816\n",
      "سندھ: 812\n",
      "گے: 799\n",
      "نواز: 721\n",
      "عمران: 699\n",
      "\n",
      "Top 10 Most Common Bigrams:\n",
      "عمران خان: 507\n",
      "نواز شریف: 459\n",
      "t co: 407\n",
      "سندھ پولیس: 295\n",
      "آرمی چیف: 227\n",
      "کیپٹن صفدر: 184\n",
      "مریم نواز: 159\n",
      "ن لیگ: 155\n",
      "اللہ علیہ: 143\n",
      "جزا اللہ: 133\n",
      "\n",
      "Top 10 Most Common Trigrams:\n",
      "صلی اللہ علیہ: 100\n",
      "پی ڈی ایم: 88\n",
      "فالو فالو بیک: 71\n",
      "جزا اللہ خیر: 66\n",
      "sarcasm t co: 65\n",
      "اللہ علیہ آلہ: 61\n",
      "علیہ آلہ وسلم: 61\n",
      "ریٹویٹ ریٹویٹ فالو: 59\n",
      "ایس ایچ او: 55\n",
      "استغفار اللهہ واتوب: 53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming the DataFrame 'df' is already loaded and the 'corrected_text' column exists\n",
    "\n",
    "# Initialize CountVectorizer for unigram, bigram, and trigram analysis\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b')\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b')\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "# Fit and transform the corrected text for unigrams, bigrams, and trigrams\n",
    "unigram_matrix = unigram_vectorizer.fit_transform(df['corrected_text'])\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(df['corrected_text'])\n",
    "trigram_matrix = trigram_vectorizer.fit_transform(df['corrected_text'])\n",
    "\n",
    "# Get feature names for each\n",
    "unigram_features = unigram_vectorizer.get_feature_names_out()\n",
    "bigram_features = bigram_vectorizer.get_feature_names_out()\n",
    "trigram_features = trigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the counts for each n-gram\n",
    "unigram_counts = unigram_matrix.sum(axis=0)\n",
    "bigram_counts = bigram_matrix.sum(axis=0)\n",
    "trigram_counts = trigram_matrix.sum(axis=0)\n",
    "\n",
    "# Create dictionaries to store n-gram frequencies\n",
    "unigram_freq = {unigram_features[i]: unigram_counts[0, i] for i in range(unigram_counts.shape[1])}\n",
    "bigram_freq = {bigram_features[i]: bigram_counts[0, i] for i in range(bigram_counts.shape[1])}\n",
    "trigram_freq = {trigram_features[i]: trigram_counts[0, i] for i in range(trigram_counts.shape[1])}\n",
    "\n",
    "# Get the top 10 most common unigrams, bigrams, and trigrams\n",
    "top_10_unigrams = Counter(unigram_freq).most_common(10)\n",
    "top_10_bigrams = Counter(bigram_freq).most_common(10)\n",
    "top_10_trigrams = Counter(trigram_freq).most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Most Common Unigrams:\")\n",
    "for unigram, freq in top_10_unigrams:\n",
    "    print(f\"{unigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Common Bigrams:\")\n",
    "for bigram, freq in top_10_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Common Trigrams:\")\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16661/16661 [00:00<00:00, 92768.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      tokenized_text\n",
      "0        [لینے, شادی, فساد, ٹھیک, کوجی, نہیں, چاہیے]\n",
      "1  [چل, مہمانوں, کھانا, سروس, چڑیل, چاچا, دنوں, د...\n",
      "2  [کامران, خان, آپکی, دن, بحریہ, زمہ, داری, لگائ...\n",
      "4              [مراد, علی, شاہ, بھیس, ڈی, ایس, حامی]\n",
      "5                       [قابل, اعتبار, قاتل, اعتبار]\n",
      "Top 5 words most similar to 'اچھا':\n",
      "کافی: 0.8639\n",
      "زرا: 0.8331\n",
      "حال: 0.8245\n",
      "پیار: 0.8230\n",
      "دکھ: 0.8211\n",
      "Word embeddings saved to 'word_embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Ensure all entries in 'lemmatize_text' column are strings and handle NaN values\n",
    "df['corrected_text'] = df['corrected_text'].fillna('').astype(str)\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenization with progress bar\n",
    "df['tokenized_text'] = df['corrected_text'].progress_apply(urdu_text_processing.urdu_tokenize)\n",
    "\n",
    "# Print the first 5 rows to check the results\n",
    "print(df[['tokenized_text']].head())\n",
    "# Train Word2Vec model on tokenized text\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Query the model for the top 5 words most similar to \"اچھا\" (good)\n",
    "similar_words = model.wv.most_similar(\"اچھا\", topn=5)\n",
    "\n",
    "# Display the similar words and their similarity scores\n",
    "print(\"Top 5 words most similar to 'اچھا':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n",
    "# Train Word2Vec model on tokenized text\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Get word embeddings\n",
    "word_vectors = model.wv\n",
    "words = word_vectors.index_to_key  # List of words in the model's vocabulary\n",
    "embeddings = [word_vectors[word] for word in words]  # Corresponding embeddings\n",
    "\n",
    "# Create a DataFrame to store words and their embeddings\n",
    "embedding_df = pd.DataFrame(embeddings, index=words)\n",
    "\n",
    "# Save embeddings to CSV\n",
    "embedding_df.to_csv('word_embeddings.csv', index_label='word')\n",
    "\n",
    "# Display success message\n",
    "print(\"Word embeddings saved to 'word_embeddings.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic,SVM AND NAIVE BYS and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16661/16661 [00:00<00:00, 89975.07it/s]\n",
      "100%|██████████| 16661/16661 [00:00<00:00, 23372.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.7354\n",
      "Precision: 0.7380\n",
      "Recall: 0.8100\n",
      "F1 Score: 0.7723\n",
      "\n",
      "Naive Bayes Metrics:\n",
      "Accuracy: 0.7195\n",
      "Precision: 0.7246\n",
      "Recall: 0.7964\n",
      "F1 Score: 0.7588\n",
      "\n",
      "SVM Metrics:\n",
      "Accuracy: 0.7462\n",
      "Precision: 0.7346\n",
      "Recall: 0.8484\n",
      "F1 Score: 0.7874\n",
      "\n",
      "Testing Logistic Regression:\n",
      "Input: 'یہ کمال کا ہے، بالکل نہیں!' -> Prediction: Sarcastic\n",
      "Input: 'کتاب واقعی اچھی ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ تو بہت برا ہے!' -> Prediction: Sarcastic\n",
      "Input: 'واہ! تم نے تو یہ کام کر کے دنیا کو حیران کر دیا!' -> Prediction: Non-Sarcastic\n",
      "Input: 'بالکل، اس منصوبے کی کامیابی کی کوئی امید نہیں!' -> Prediction: Sarcastic\n",
      "Input: 'کیا بات ہے، تمہاری عقل تو بہت ہی چمکدار ہے!' -> Prediction: Sarcastic\n",
      "Input: 'یہ ہے واقعی بہترین خیال، جیسے چائے میں نمک ملانا!' -> Prediction: Non-Sarcastic\n",
      "Input: 'تمہارے ساتھ گفتگو کرنا واقعی ایک خوشی کی بات ہے!' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ کتاب بہت معلوماتی ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ تصویر بہت خوبصورت ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'مجھے تمہاری مدد کی ضرورت ہے۔' -> Prediction: Sarcastic\n",
      "Input: 'آج کا موسم بہت اچھا ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'میں نے آج ایک نئی ترکیب سیکھی ہے۔' -> Prediction: Non-Sarcastic\n",
      "\n",
      "Testing Naive Bayes:\n",
      "Input: 'یہ کمال کا ہے، بالکل نہیں!' -> Prediction: Non-Sarcastic\n",
      "Input: 'کتاب واقعی اچھی ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ تو بہت برا ہے!' -> Prediction: Non-Sarcastic\n",
      "Input: 'واہ! تم نے تو یہ کام کر کے دنیا کو حیران کر دیا!' -> Prediction: Non-Sarcastic\n",
      "Input: 'بالکل، اس منصوبے کی کامیابی کی کوئی امید نہیں!' -> Prediction: Non-Sarcastic\n",
      "Input: 'کیا بات ہے، تمہاری عقل تو بہت ہی چمکدار ہے!' -> Prediction: Sarcastic\n",
      "Input: 'یہ ہے واقعی بہترین خیال، جیسے چائے میں نمک ملانا!' -> Prediction: Non-Sarcastic\n",
      "Input: 'تمہارے ساتھ گفتگو کرنا واقعی ایک خوشی کی بات ہے!' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ کتاب بہت معلوماتی ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ تصویر بہت خوبصورت ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'مجھے تمہاری مدد کی ضرورت ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'آج کا موسم بہت اچھا ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'میں نے آج ایک نئی ترکیب سیکھی ہے۔' -> Prediction: Non-Sarcastic\n",
      "\n",
      "Testing SVM:\n",
      "Input: 'یہ کمال کا ہے، بالکل نہیں!' -> Prediction: Non-Sarcastic\n",
      "Input: 'کتاب واقعی اچھی ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ تو بہت برا ہے!' -> Prediction: Sarcastic\n",
      "Input: 'واہ! تم نے تو یہ کام کر کے دنیا کو حیران کر دیا!' -> Prediction: Non-Sarcastic\n",
      "Input: 'بالکل، اس منصوبے کی کامیابی کی کوئی امید نہیں!' -> Prediction: Sarcastic\n",
      "Input: 'کیا بات ہے، تمہاری عقل تو بہت ہی چمکدار ہے!' -> Prediction: Sarcastic\n",
      "Input: 'یہ ہے واقعی بہترین خیال، جیسے چائے میں نمک ملانا!' -> Prediction: Non-Sarcastic\n",
      "Input: 'تمہارے ساتھ گفتگو کرنا واقعی ایک خوشی کی بات ہے!' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ کتاب بہت معلوماتی ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ تصویر بہت خوبصورت ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'مجھے تمہاری مدد کی ضرورت ہے۔' -> Prediction: Sarcastic\n",
      "Input: 'آج کا موسم بہت اچھا ہے۔' -> Prediction: Non-Sarcastic\n",
      "Input: 'میں نے آج ایک نئی ترکیب سیکھی ہے۔' -> Prediction: Non-Sarcastic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB  # Use Gaussian Naive Bayes\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure all entries in 'corrected_text' are strings and handle NaN values\n",
    "df['corrected_text'] = df['corrected_text'].fillna('').astype(str)\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenization with progress bar\n",
    "df['tokenized_text'] = df['corrected_text'].progress_apply(urdu_text_processing.urdu_tokenize)\n",
    "\n",
    "# Train Word2Vec model on tokenized text\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=100, window=5, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Function to create feature vectors by averaging word embeddings\n",
    "def get_feature_vector(tokens, model):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[valid_tokens], axis=0)\n",
    "\n",
    "# Create feature vectors for the entire dataset\n",
    "df['feature_vectors'] = df['tokenized_text'].progress_apply(lambda x: get_feature_vector(x, model))\n",
    "\n",
    "# Create a DataFrame for features and labels\n",
    "X = np.array(list(df['feature_vectors']))\n",
    "y = df['is_sarcastic'].values  # Ensure 'is_sarcastic' is your actual label column\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(classifier, X_train, y_train, X_test, y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_classifier = LogisticRegression(max_iter=1000)\n",
    "logistic_metrics = evaluate_model(logistic_classifier, X_train, y_train, X_test, y_test)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {logistic_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {logistic_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {logistic_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {logistic_metrics[3]:.4f}\\n\")\n",
    "\n",
    "# Naive Bayes\n",
    "naive_bayes_classifier = GaussianNB()  # Use Gaussian Naive Bayes\n",
    "naive_bayes_metrics = evaluate_model(naive_bayes_classifier, X_train, y_train, X_test, y_test)\n",
    "print(\"Naive Bayes Metrics:\")\n",
    "print(f\"Accuracy: {naive_bayes_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {naive_bayes_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {naive_bayes_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {naive_bayes_metrics[3]:.4f}\\n\")\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_classifier = SVC()\n",
    "svm_metrics = evaluate_model(svm_classifier, X_train, y_train, X_test, y_test)\n",
    "print(\"SVM Metrics:\")\n",
    "print(f\"Accuracy: {svm_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {svm_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {svm_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {svm_metrics[3]:.4f}\\n\")\n",
    "\n",
    "# Testing the models with sample inputs\n",
    "def test_model(classifier, samples):\n",
    "    for text in samples:\n",
    "        tokens = urdu_text_processing.urdu_tokenize(text)\n",
    "        feature_vector = get_feature_vector(tokens, model).reshape(1, -1)  # Reshape for prediction\n",
    "        prediction = classifier.predict(feature_vector)\n",
    "        sentiment = \"Sarcastic\" if prediction[0] == 1 else \"Non-Sarcastic\"\n",
    "        print(f\"Input: '{text}' -> Prediction: {sentiment}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Logistic Regression:\n",
      "Input: 'کتنے اچھے ہیں یہ لوگ جو ہر وقت میرا مذاق اڑاتے ہیں :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'آج موسم بہت خوشگوار ہے۔ :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ کھانا بہت مزیدار ہے :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'واقعی بہت مزہ آیا، تم نے مزے کا بیڑا غرق کر دیا - :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'کیا زبردست فیصلہ تھا، سب کچھ برباد ہو گیا :Sentence' -> Prediction: Sarcastic\n",
      "\n",
      "Testing Naive Bayes:\n",
      "Input: 'کتنے اچھے ہیں یہ لوگ جو ہر وقت میرا مذاق اڑاتے ہیں :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'آج موسم بہت خوشگوار ہے۔ :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ کھانا بہت مزیدار ہے :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'واقعی بہت مزہ آیا، تم نے مزے کا بیڑا غرق کر دیا - :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'کیا زبردست فیصلہ تھا، سب کچھ برباد ہو گیا :Sentence' -> Prediction: Sarcastic\n",
      "\n",
      "Testing SVM:\n",
      "Input: 'کتنے اچھے ہیں یہ لوگ جو ہر وقت میرا مذاق اڑاتے ہیں :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'آج موسم بہت خوشگوار ہے۔ :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'یہ کھانا بہت مزیدار ہے :Sentence' -> Prediction: Non-Sarcastic\n",
      "Input: 'واقعی بہت مزہ آیا، تم نے مزے کا بیڑا غرق کر دیا - :Sentence' -> Prediction: Sarcastic\n",
      "Input: 'کیا زبردست فیصلہ تھا، سب کچھ برباد ہو گیا :Sentence' -> Prediction: Sarcastic\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"کتنے اچھے ہیں یہ لوگ جو ہر وقت میرا مذاق اڑاتے ہیں :Sentence\",  # Sarcastic\n",
    "    \"آج موسم بہت خوشگوار ہے۔ :Sentence\",  # Non-Sarcastic\n",
    "    \"یہ کھانا بہت مزیدار ہے :Sentence\",  # Non-Sarcastic\n",
    "    \"واقعی بہت مزہ آیا، تم نے مزے کا بیڑا غرق کر دیا - :Sentence\",  # Sarcastic\n",
    "    \"کیا زبردست فیصلہ تھا، سب کچھ برباد ہو گیا :Sentence\"  # Sarcastic\n",
    "]\n",
    "print(\"Testing Logistic Regression:\")\n",
    "test_model(logistic_classifier, sample_texts)\n",
    "print(\"\\nTesting Naive Bayes:\")\n",
    "test_model(naive_bayes_classifier, sample_texts)\n",
    "print(\"\\nTesting SVM:\")\n",
    "test_model(svm_classifier, sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
